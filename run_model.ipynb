{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '-i \"data/sic_hierarchy\" -o \"woop\" -m \"xmlcnn\" --mode \"cat\" -l \"masked_categorical\" --epoch 1'.replace('\"','').split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import argparse\n",
    "import os,datetime\n",
    "\n",
    "# save things\n",
    "import pandas as pd\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "# model_func\n",
    "from tools.model_func import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARGPARSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description = 'run baseline models')\n",
    "parser.add_argument('-i','--input', required = True, type = str, help = 'input directory e.g. ./data/dl_amazon_1/')\n",
    "parser.add_argument('-m','--model', required = True, type = str, help = 'model, one in: xmlcnn, attentionxml, attention,')\n",
    "parser.add_argument('-l','--loss', required = True, type = str, help = \"loss type, categorical or binary \")\n",
    "parser.add_argument('-o','--output', required = True, type = str, help = 'output directory')\n",
    "parser.add_argument('--mode', required = True, type = str, help = 'cat,hierarchy')\n",
    "parser.add_argument('--epoch', default = 5, type = int, help = 'epochs')\n",
    "parser.add_argument('--batch_size', default = 0, type = int, help = 'batch size')\n",
    "parser.add_argument('--save_weights', default = True, action = 'store_true', help = 'save trained weights')\n",
    "parser.add_argument('--save_model', default = True, action = 'store_true', help = 'save trained model architecture')\n",
    "parser.add_argument('--save_prediction', default = True, action = 'store_true', help = 'save top 10 prediction and corresponding probabilities (not implemented yet)')\n",
    "args = parser.parse_args(s)\n",
    "\n",
    "# argparse validation\n",
    "default_batch_size = {'xmlcnn':128,'attentionxml':20,'attention':25}\n",
    "if not os.path.exists(args.input):\n",
    "    raise Exception('Input path does not exist: {}'.format(args.input))\n",
    "if args.model not in default_batch_size.keys():\n",
    "    raise Exception('Unknown model: {}'.format(args.model))\n",
    "if args.loss not in ['binary','categorical','masked_categorical']:\n",
    "    raise Exception('Unknown loss: {}'.format(args.loss))\n",
    "if args.mode not in ['cat','hierarchy']:\n",
    "    raise Exception('Unknown mode: {}'.format(args.mode))\n",
    "\n",
    "IN_DIR = args.input\n",
    "if not args.batch_size:\n",
    "    args.batch_size = default_batch_size[args.model]\n",
    "if not os.path.exists(args.output):\n",
    "    os.mkdir(args.output)\n",
    "    print(Coloured(\"Create Output Directory: {}\".format(args.output)))\n",
    "OUT_DIR = os.path.join(\n",
    "    args.output,\n",
    "    datetime.datetime.now().strftime('%y%m%d_%H%M%S_{}'.format(args.model)),\n",
    ")\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.mkdir(OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data input\n",
    "import numpy as np\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_ys\n",
    "import pickle\n",
    "def mask_ys(y_trues,in_dir):\n",
    "    # slow an stupid way to mask non important values to -1\n",
    "    ds = pickle.load(open(os.path.join(in_dir,'child_to_siblings.pkl'),'rb'))\n",
    "    outs = [y_trues[0]]\n",
    "    for i,y in enumerate(y_trues):\n",
    "        if i==0:\n",
    "            continue\n",
    "        d = ds[i]\n",
    "        y_true = y.argmax(axis=1).flatten()\n",
    "        out = -np.ones(shape=y.shape)\n",
    "        for j,yt in enumerate(y_true):\n",
    "            out[j,d[yt]]=0\n",
    "            out[j,yt]=1\n",
    "        outs.append(out)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric and loss function\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.metrics import categorical_accuracy, binary_accuracy, top_k_categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_categorical_cross_entropy_with_logits(y_true, y_pred):\n",
    "    y_pred = tf.where(K.not_equal(y_true, -1), y_pred, -1e7*tf.ones_like(y_pred))\n",
    "    loss = K.categorical_crossentropy(tf.maximum(y_true,0.) ,y_pred, from_logits=True)\n",
    "    return K.mean(loss,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(os.path.join(OUT_DIR,'train.log'),append=False)\n",
    "# inputs\n",
    "x_train,y_trains,x_test,y_tests = get_input(IN_DIR,args.mode)\n",
    "if args.loss.startswith('masked'):\n",
    "    y_trains = mask_ys(y_trains,IN_DIR)\n",
    "    y_tests = mask_ys(y_tests,IN_DIR)\n",
    "_,max_sequence_length = x_train.shape\n",
    "labels_dims = [l.shape[-1] for l in y_tests]\n",
    "# model\n",
    "embedding_layer = get_embedding_layer(IN_DIR)\n",
    "model = get_model(model_name = args.model,\n",
    "                  max_sequence_length = max_sequence_length,\n",
    "                  labels_dims = labels_dims,\n",
    "                  embedding_layer = embedding_layer)\n",
    "loss_dict = {'binary':binary_cross_entropy_with_logits,\n",
    "             'categorical':categorical_cross_entropy_with_logits,\n",
    "             'masked_categorical':masked_categorical_cross_entropy_with_logits,\n",
    "            }\n",
    "# train \n",
    "model.compile(loss = loss_dict[args.loss],\n",
    "              optimizer = 'adam',\n",
    "              metrics = [pAt1,pAt5])\n",
    "model.fit(x_train, y_trains,\n",
    "          batch_size = args.batch_size,\n",
    "          epochs = args.epoch,\n",
    "          validation_data = (x_test, y_tests),\n",
    "          callbacks = [csv_logger],\n",
    "          shuffle = True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.save_weights:\n",
    "    model.save_weights(os.path.join(OUT_DIR,'weights.h5'))\n",
    "if args.save_model:\n",
    "    with open(os.path.join(OUT_DIR,'model.json'),'w') as f:\n",
    "        f.write(model.to_json())\n",
    "if args.save_prediction:\n",
    "    save_predictions(model,x_test,y_tests,OUT_DIR)\n",
    "pd.DataFrame.from_dict([vars(args)]).to_csv(os.path.join(OUT_DIR,'args.csv'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
