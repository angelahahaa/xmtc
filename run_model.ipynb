{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '-i data/sic_hierarchy -m xmlcnn -o temp -l binary'.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import argparse\n",
    "import os,datetime\n",
    "# data input\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "# metric and loss function\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.metrics import categorical_accuracy, binary_accuracy, top_k_categorical_accuracy\n",
    "# embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Embedding\n",
    "# models\n",
    "from keras.layers import Dense, Input, Flatten, Concatenate, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, TimeDistributed, Lambda, Softmax\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "# misc\n",
    "from tools.MyClock import MyClock\n",
    "# save things\n",
    "import pandas as pd\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Coloured(string):\n",
    "    return \"\\033[1;30;43m {} \\033[0m\".format(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARGPARSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description = 'run baseline models')\n",
    "parser.add_argument('-i','--input', required = True, type = str, help = 'input directory e.g. ./data/dl_amazon_1/')\n",
    "parser.add_argument('-m','--model', required = True, type = str, help = 'model, one in: xmlcnn, attentionxml, attention,')\n",
    "parser.add_argument('-l','--loss', required = True, type = str, help = \"loss type, categorical or binary \")\n",
    "parser.add_argument('-o','--output', required = True, type = str, help = 'output directory')\n",
    "parser.add_argument('--mode', required = True, type = str, help = 'cat,hierarchy')\n",
    "parser.add_argument('--epoch', default = 5, type = int, help = 'epochs')\n",
    "parser.add_argument('--batch_size', default = 0, type = int, help = 'batch size')\n",
    "parser.add_argument('--save_weights', default = True, action = 'store_true', help = 'save trained weights')\n",
    "parser.add_argument('--save_model', default = True, action = 'store_true', help = 'save trained model architecture')\n",
    "parser.add_argument('--save_prediction', default = True, action = 'store_true', help = 'save top 10 prediction and corresponding probabilities (not implemented yet)')\n",
    "args = parser.parse_args(s)\n",
    "\n",
    "# argparse validation\n",
    "default_batch_size = {'xmlcnn':128,'attentionxml':20,'attention':25}\n",
    "if not os.path.exists(args.input):\n",
    "    raise Exception('Input path does not exist: {}'.format(args.input))\n",
    "if args.model not in default_batch_size.keys():\n",
    "    raise Exception('Unknown model: {}'.format(args.model))\n",
    "if args.loss not in ['binary','categorical']:\n",
    "    raise Exception('Unknown loss: {}'.format(args.loss))\n",
    "if args.mode not in ['caat','hierarchy']:\n",
    "    raise Exception('Unknown mode: {}'.formate(args.mode))\n",
    "\n",
    "IN_DIR = args.input\n",
    "if not args.batch_size:\n",
    "    args.batch_size = default_batch_size[args.model]\n",
    "if not os.path.exists(args.output):\n",
    "    os.mkdir(args.output)\n",
    "    print(Coloured(\"Create Output Directory: {}\".format(args.output)))\n",
    "OUT_DIR = os.path.join(\n",
    "    args.output,\n",
    "    datetime.datetime.now().strftime('%y%m%d_%H%M%S_{}'.format(args.model)),\n",
    ")\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.mkdir(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(in_dir,mode):\n",
    "    x_train = np.load(os.path.join(in_dir,'x_train.npy'))\n",
    "    dirs = [os.path.join(in_dir,d) for d in sorted(os.listdir(in_dir)) if d.startswith('y_train_{}'.format(mode))]\n",
    "    y_trains = [scipy.sparse.load_npz(d).todense() for d in dirs]\n",
    "\n",
    "    x_test = np.load(os.path.join(in_dir,'x_test.npy'))\n",
    "    dirs = [os.path.join(in_dir,d) for d in sorted(os.listdir(in_dir)) if d.startswith('y_test_{}'.format(mode))]\n",
    "    y_tests = [scipy.sparse.load_npz(d).todense() for d in dirs]\n",
    "    return x_train,y_trains,x_test,y_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBEDDING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_layer():\n",
    "    embedding_matrix = np.load(os.path.join(IN_DIR,'embedding_matrix.npy'))\n",
    "    num_words, embedding_dim = embedding_matrix.shape\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                embeddings_initializer=Constant(embedding_matrix),\n",
    "                                trainable=False)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_attention(inputs):\n",
    "    input1, input2 = inputs\n",
    "    outer_product = tf.einsum('ghj, ghk -> gjk', input1, input2)\n",
    "    return outer_product\n",
    "\n",
    "def get_model(model_name, max_sequence_length, labels_dims):\n",
    "    embedding_layer = get_embedding_layer()\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    if model_name == 'xmlcnn':\n",
    "        filter_sizes = [2,4,8]\n",
    "        pooling_units = 32\n",
    "        bottle_neck = 512\n",
    "        convs = []\n",
    "        for fsz in filter_sizes:\n",
    "            l = Conv1D(filters = 128, kernel_size = fsz, strides = 2, activation = 'relu')(embedded_sequences)\n",
    "            s = int(l.shape[-2])\n",
    "            pool_size = s//pooling_units\n",
    "            l = MaxPooling1D(pool_size,padding = 'same')(l)\n",
    "            l = Flatten()(l)\n",
    "            convs.append(l)\n",
    "        x = Concatenate(axis=-1)(convs)\n",
    "        x = Dense(bottle_neck, activation = 'relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        outs = []\n",
    "        for i,labels_dim in enumerate(labels_dims):\n",
    "            outs.append(Dense(labels_dim, activation = None, name = 'H{}'.format(i))(x))\n",
    "    elif model_name in ['attentionxml','attention']:\n",
    "        labels_dim = sum(labels_dims)\n",
    "        if model_name == 'attentionxml':\n",
    "            # with lstm\n",
    "            x = Bidirectional(CuDNNLSTM(512, return_sequences=True))(embedded_sequences)\n",
    "        else:\n",
    "            # without lstm\n",
    "            x = embedded_sequences\n",
    "        attention = Dense(labels_dim,activation=None,name='attention_dense')(x)\n",
    "        attention = Softmax(axis=1,name='attention_softmax')(attention)\n",
    "        x = Lambda(apply_attention,name = 'apply_attention')([x, attention])\n",
    "        x = Lambda(lambda x:K.permute_dimensions(x,(0,2,1)),name='transpose')(x)\n",
    "        x = TimeDistributed(Dense(512,activation='relu'))(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = TimeDistributed(Dense(256,activation='relu'))(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = TimeDistributed(Dense(1,activation=None))(x)\n",
    "        x = Lambda(lambda x:K.squeeze(x,axis=-1))(x)\n",
    "        outs = []\n",
    "        start = 0\n",
    "        for i,labels_dim in enumerate(labels_dims):\n",
    "            outs.append(Lambda(lambda x:x[:,start:start+labels_dim],name = 'H{}'.format(i))(x))\n",
    "            start+=labels_dim\n",
    "    else:\n",
    "        raise Exception('Invalid model_name : {}'.format(model_name))\n",
    "    return Model(sequence_input, outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_with_logits(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_true,y_pred,from_logits=True),axis=-1)\n",
    "def categorical_cross_entropy_with_logits(y_true, y_pred):\n",
    "    return K.mean(K.categorical_crossentropy(y_true,y_pred,from_logits=True),axis=-1)\n",
    "loss_dict = {'binary':binary_cross_entropy_with_logits,\n",
    "             'categorical':categorical_cross_entropy_with_logits,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy_with_logits(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.tf.cast(K.less(0.0,y_pred), y_true.dtype)))\n",
    "def pAt1(y_true,y_pred):\n",
    "    return categorical_accuracy(y_true, y_pred)\n",
    "def pAt5(y_true,y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(model,x_test,y_tests,out_dir):\n",
    "    print('SAVE PREDICTIONS TO : {}'.format(out_dir))\n",
    "    out_probs = model.predict(x_test,verbose=1)\n",
    "    ind_dirs = [os.path.join(out_dir,'pred_outputs{}.txt'.format(i)) for i in range(len(y_tests))]\n",
    "    log_dirs = [os.path.join(out_dir,'pred_logits{}.txt'.format(i)) for i in range(len(y_tests))]\n",
    "    f_ind = [open(ind_dir,'ab') for ind_dir in ind_dirs]\n",
    "    f_log = [open(log_dir,'ab') for log_dir in log_dirs]\n",
    "    for i,out_prob in enumerate(out_probs):\n",
    "        ind = np.argsort(out_prob,axis=1)[:,:-11:-1]\n",
    "        logits = np.take_along_axis(out_prob, ind, axis=1)\n",
    "        np.savetxt(f_ind[i],ind,fmt='%d')\n",
    "        np.savetxt(f_log[i],logits,fmt='%1.3f')\n",
    "    for f in f_ind + f_log:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0725 15:08:10.132068 140424970528576 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0725 15:08:10.142550 140424970528576 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0725 15:08:10.441274 140424970528576 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0725 15:08:10.456938 140424970528576 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0725 15:08:10.513536 140424970528576 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0725 15:08:10.519779 140424970528576 deprecation.py:506] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0725 15:08:10.564301 140424970528576 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0725 15:08:10.580689 140424970528576 deprecation.py:323] From /home/angela/env/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0725 15:08:11.112566 140424970528576 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 572101 samples, validate on 142756 samples\n",
      "Epoch 1/5\n",
      "572101/572101 [==============================] - 94s 165us/step - loss: 0.1576 - H0_loss: 0.1025 - H1_loss: 0.0351 - H2_loss: 0.0106 - H3_loss: 0.0094 - H0_pAt1: 0.6588 - H0_pAt5: 0.9062 - H1_pAt1: 0.5498 - H1_pAt5: 0.7786 - H2_pAt1: 0.3431 - H2_pAt5: 0.5779 - H3_pAt1: 0.3219 - H3_pAt5: 0.5583 - val_loss: 0.1338 - val_H0_loss: 0.0905 - val_H1_loss: 0.0292 - val_H2_loss: 0.0075 - val_H3_loss: 0.0066 - val_H0_pAt1: 0.6962 - val_H0_pAt5: 0.9310 - val_H1_pAt1: 0.6123 - val_H1_pAt5: 0.8408 - val_H2_pAt1: 0.4577 - val_H2_pAt5: 0.7001 - val_H3_pAt1: 0.4341 - val_H3_pAt5: 0.6863\n",
      "Epoch 2/5\n",
      "572101/572101 [==============================] - 90s 158us/step - loss: 0.1351 - H0_loss: 0.0906 - H1_loss: 0.0298 - H2_loss: 0.0078 - H3_loss: 0.0068 - H0_pAt1: 0.7024 - H0_pAt5: 0.9299 - H1_pAt1: 0.6113 - H1_pAt5: 0.8356 - H2_pAt1: 0.4399 - H2_pAt5: 0.6880 - H3_pAt1: 0.4168 - H3_pAt5: 0.6736 - val_loss: 0.1316 - val_H0_loss: 0.0896 - val_H1_loss: 0.0285 - val_H2_loss: 0.0072 - val_H3_loss: 0.0063 - val_H0_pAt1: 0.6997 - val_H0_pAt5: 0.9323 - val_H1_pAt1: 0.6191 - val_H1_pAt5: 0.8482 - val_H2_pAt1: 0.4735 - val_H2_pAt5: 0.7194 - val_H3_pAt1: 0.4520 - val_H3_pAt5: 0.7066\n",
      "Epoch 3/5\n",
      "572101/572101 [==============================] - 90s 158us/step - loss: 0.1290 - H0_loss: 0.0863 - H1_loss: 0.0286 - H2_loss: 0.0075 - H3_loss: 0.0066 - H0_pAt1: 0.7160 - H0_pAt5: 0.9375 - H1_pAt1: 0.6260 - H1_pAt5: 0.8484 - H2_pAt1: 0.4573 - H2_pAt5: 0.7083 - H3_pAt1: 0.4344 - H3_pAt5: 0.6940 - val_loss: 0.1308 - val_H0_loss: 0.0890 - val_H1_loss: 0.0283 - val_H2_loss: 0.0071 - val_H3_loss: 0.0063 - val_H0_pAt1: 0.7032 - val_H0_pAt5: 0.9332 - val_H1_pAt1: 0.6248 - val_H1_pAt5: 0.8508 - val_H2_pAt1: 0.4826 - val_H2_pAt5: 0.7250 - val_H3_pAt1: 0.4613 - val_H3_pAt5: 0.7127\n",
      "Epoch 4/5\n",
      "572101/572101 [==============================] - 91s 158us/step - loss: 0.1242 - H0_loss: 0.0828 - H1_loss: 0.0277 - H2_loss: 0.0073 - H3_loss: 0.0065 - H0_pAt1: 0.7265 - H0_pAt5: 0.9437 - H1_pAt1: 0.6357 - H1_pAt5: 0.8580 - H2_pAt1: 0.4678 - H2_pAt5: 0.7193 - H3_pAt1: 0.4449 - H3_pAt5: 0.7051 - val_loss: 0.1319 - val_H0_loss: 0.0899 - val_H1_loss: 0.0285 - val_H2_loss: 0.0072 - val_H3_loss: 0.0063 - val_H0_pAt1: 0.7033 - val_H0_pAt5: 0.9329 - val_H1_pAt1: 0.6270 - val_H1_pAt5: 0.8510 - val_H2_pAt1: 0.4873 - val_H2_pAt5: 0.7260 - val_H3_pAt1: 0.4663 - val_H3_pAt5: 0.7149\n",
      "Epoch 5/5\n",
      "572101/572101 [==============================] - 91s 158us/step - loss: 0.1201 - H0_loss: 0.0797 - H1_loss: 0.0269 - H2_loss: 0.0072 - H3_loss: 0.0064 - H0_pAt1: 0.7363 - H0_pAt5: 0.9487 - H1_pAt1: 0.6437 - H1_pAt5: 0.8658 - H2_pAt1: 0.4743 - H2_pAt5: 0.7268 - H3_pAt1: 0.4511 - H3_pAt5: 0.7125 - val_loss: 0.1332 - val_H0_loss: 0.0909 - val_H1_loss: 0.0287 - val_H2_loss: 0.0072 - val_H3_loss: 0.0064 - val_H0_pAt1: 0.7046 - val_H0_pAt5: 0.9320 - val_H1_pAt1: 0.6280 - val_H1_pAt5: 0.8513 - val_H2_pAt1: 0.4854 - val_H2_pAt5: 0.7274 - val_H3_pAt1: 0.4646 - val_H3_pAt5: 0.7152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6ac1f4390>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_logger = CSVLogger(os.path.join(OUT_DIR,'train.log'),append=False)\n",
    "# inputs\n",
    "x_train,y_trains,x_test,y_tests = get_input(IN_DIR,args.mode)\n",
    "_,max_sequence_length = x_train.shape\n",
    "labels_dims = [l.shape[-1] for l in y_tests]\n",
    "model = get_model(args.model, max_sequence_length, labels_dims)\n",
    "model.compile(loss = loss_dict[args.loss],\n",
    "              optimizer = 'adam',\n",
    "              metrics = [pAt1,pAt5])\n",
    "model.fit(x_train, y_trains,\n",
    "          batch_size = args.batch_size,\n",
    "          epochs = args.epoch,\n",
    "          validation_data = (x_test, y_tests),\n",
    "          callbacks = [csv_logger],\n",
    "          shuffle = True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE PREDICTIONS TO : temp/190725_150715_xmlcnn\n",
      "142756/142756 [==============================] - 13s 88us/step\n"
     ]
    }
   ],
   "source": [
    "if args.save_weights:\n",
    "    model.save_weights(os.path.join(OUT_DIR,'weights.h5'))\n",
    "if args.save_model:\n",
    "    with open(os.path.join(OUT_DIR,'model.json'),'w') as f:\n",
    "        f.write(model.to_json())\n",
    "if args.save_prediction:\n",
    "    save_predictions(model,x_test,y_tests,OUT_DIR)\n",
    "pd.DataFrame.from_dict([vars(args)]).to_csv(os.path.join(OUT_DIR,'args.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
