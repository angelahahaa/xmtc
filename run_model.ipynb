{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '-i \"data/sic_hierarchy\" -o \"temp_bert\" -m \"xmlcnn\" --mode \"cat\" -l \"categorical\" --gpu \"0\" --bert_bottle_neck 512 --bert_trainable_layers 3 --epoch 1 --batch_size 256 --val'.replace('\"','').split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 15:30:00.865283 140245800064832 deprecation_wrapper.py:119] From /home/angela/xmtc/tools/model_func.py:21: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import argparse\n",
    "import os,datetime\n",
    "\n",
    "# save things\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "# model_func\n",
    "from tools.model_func import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARGPARSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43m USE GPU: 0 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description = 'run baseline models')\n",
    "parser.add_argument('-i','--input', required = True, type = str, help = 'input directory e.g. ./data/dl_amazon_1/')\n",
    "parser.add_argument('-m','--model', required = True, type = str, help = 'model, one in: xmlcnn, attentionxml, attention,')\n",
    "parser.add_argument('-l','--loss', required = True, type = str, help = \"loss type: categorical, binary, masked_categorical \")\n",
    "parser.add_argument('-o','--output', required = True, type = str, help = 'output directory')\n",
    "parser.add_argument('--mode', required = True, type = str, help = 'cat,hierarchy')\n",
    "parser.add_argument('--epoch', default = 5, type = int, help = 'epochs')\n",
    "parser.add_argument('--batch_size', default = 0, type = int, help = 'batch size')\n",
    "parser.add_argument('--save_weights', default = True, action = 'store_true', help = 'save trained weights')\n",
    "parser.add_argument('--save_model', default = True, action = 'store_true', help = 'save trained model architecture')\n",
    "parser.add_argument('--save_prediction', default = True, action = 'store_true', help = 'save top 10 prediction and corresponding probabilities (not implemented yet)')\n",
    "parser.add_argument('--gpu', default = '', type = str, help = 'GPU id to use')\n",
    "parser.add_argument('--bert_bottle_neck', default = 512, type = int, help = 'bottle neck dim for bert, 0 implies no bottle neck layer')\n",
    "parser.add_argument('--bert_trainable_layers', default = 10, type = int, help = 'number of trainable layers in bert ')\n",
    "parser.add_argument('--val', default = False, action = 'store_true',help = 'use validation set')\n",
    "args = parser.parse_args(s)\n",
    "\n",
    "# argparse validation\n",
    "default_batch_size = {'xmlcnn':128,'attentionxml':20,'attention':25,'bert':256,}\n",
    "if not os.path.exists(args.input):\n",
    "    raise Exception('Input path does not exist: {}'.format(args.input))\n",
    "if args.model not in default_batch_size.keys():\n",
    "    raise Exception('Unknown model: {}'.format(args.model))\n",
    "if args.loss not in ['binary','categorical','masked_categorical']:\n",
    "    raise Exception('Unknown loss: {}'.format(args.loss))\n",
    "if args.mode not in ['cat','hierarchy']:\n",
    "    raise Exception('Unknown mode: {}'.format(args.mode))\n",
    "\n",
    "IN_DIR = args.input\n",
    "if not args.batch_size:\n",
    "    args.batch_size = default_batch_size[args.model]\n",
    "if not os.path.exists(args.output):\n",
    "    os.mkdir(args.output)\n",
    "    print(Coloured(\"Create Output Directory: {}\".format(args.output)))\n",
    "OUT_DIR = os.path.join(\n",
    "    args.output,\n",
    "    datetime.datetime.now().strftime('%y%m%d_%H%M%S_{}'.format(args.model)),\n",
    ")\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.mkdir(OUT_DIR)\n",
    "\n",
    "if args.gpu:\n",
    "    print(Coloured(\"USE GPU: {}\".format(args.gpu)))\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD EXISTING VAL INDS\n"
     ]
    }
   ],
   "source": [
    "if args.model == 'bert':\n",
    "    x_trains,y_trains,x_tests,y_tests = get_bert_input(IN_DIR,args.mode)\n",
    "else:\n",
    "    x_trains,y_trains,x_tests,y_tests = get_input(IN_DIR,args.mode)\n",
    "if args.loss.startswith('masked'):\n",
    "    print(Coloured(\"MASKING INPUT\"))\n",
    "    y_trains = mask_ys(y_trains,IN_DIR)\n",
    "    y_tests = mask_ys(y_tests,IN_DIR)\n",
    "max_sequence_length = len(x_trains[0][0])\n",
    "labels_dims = [l.shape[-1] for l in y_tests]\n",
    "\n",
    "if args.val:\n",
    "    x_trains,y_trains,x_vals,y_vals = get_unbiased_train_val_split(x_trains,y_trains,IN_DIR)\n",
    "else:\n",
    "    x_vals,y_vals = x_tests,y_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = {'binary':binary_cross_entropy_with_logits,\n",
    "             'categorical':categorical_cross_entropy_with_logits,\n",
    "             'masked_categorical':masked_categorical_cross_entropy_with_logits,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0812 15:30:13.389801 140245800064832 deprecation.py:506] From /home/angela/env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 300)     15000300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 50, 128)      76928       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 49, 128)      153728      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 47, 128)      307328      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 50, 128)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 49, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 47, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 6400)         0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6272)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 6016)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 18688)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          9568768     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "H0 (Dense)                      (None, 18)           9234        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "H1 (Dense)                      (None, 77)           39501       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "H2 (Dense)                      (None, 453)          232389      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "H3 (Dense)                      (None, 538)          275994      dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 25,664,170\n",
      "Trainable params: 10,663,870\n",
      "Non-trainable params: 15,000,300\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1831116 samples, validate on 457296 samples\n",
      "1831116/1831116 [==============================] - 213s 116us/sample - loss: 7.6262 - H0_loss: 1.1163 - H1_loss: 1.5875 - H2_loss: 2.4059 - H3_loss: 2.5166 - H0_pAt1: 0.6767 - H0_pAt5: 0.9236 - H1_pAt1: 0.6061 - H1_pAt5: 0.8379 - H2_pAt1: 0.4777 - H2_pAt5: 0.7161 - H3_pAt1: 0.4567 - H3_pAt5: 0.7032 - val_loss: 6.2066 - val_H0_loss: 0.9168 - val_H1_loss: 1.2927 - val_H2_loss: 1.9524 - val_H3_loss: 2.0445 - val_H0_pAt1: 0.7254 - val_H0_pAt5: 0.9493 - val_H1_pAt1: 0.6599 - val_H1_pAt5: 0.8862 - val_H2_pAt1: 0.5430 - val_H2_pAt5: 0.7914 - val_H3_pAt1: 0.5226 - val_H3_pAt5: 0.7811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ccc263208>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define callbacks\n",
    "callbacks = []\n",
    "\n",
    "csv_log_dir = os.path.join(OUT_DIR,'train.log')\n",
    "callbacks.append(CSVLogger(csv_log_dir,append=False))\n",
    "# build model\n",
    "sess = tf.Session()\n",
    "if args.model == 'bert':\n",
    "    model = get_bert_model(max_sequence_length, labels_dims,\n",
    "                        bottle_neck = args.bert_bottle_neck,\n",
    "                        trainable_layers = args.bert_trainable_layers,\n",
    "                        sess = sess,\n",
    "                        )\n",
    "else:\n",
    "    embedding_layer = get_embedding_layer(IN_DIR)\n",
    "    model = get_model(model_name = args.model,\n",
    "                      max_sequence_length = max_sequence_length,\n",
    "                      labels_dims = labels_dims,\n",
    "                      embedding_layer = embedding_layer)\n",
    "# print summary\n",
    "model.summary()\n",
    "\n",
    "# compile\n",
    "if args.model == 'bert':\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "else:\n",
    "    optimizer = 'adam'\n",
    "model.compile(loss = loss_dict[args.loss],\n",
    "              optimizer = optimizer,\n",
    "              metrics = [pAt1,pAt5])\n",
    "# train\n",
    "model.fit(x_trains, y_trains,\n",
    "          batch_size = args.batch_size,\n",
    "          epochs = args.epoch,\n",
    "          validation_data = (x_vals, y_vals),\n",
    "          callbacks = callbacks,\n",
    "          shuffle = True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142756/142756 [==============================] - 27s 187us/sample - loss: 7.0576 - H0_loss: 1.0548 - H1_loss: 1.4834 - H2_loss: 2.2110 - H3_loss: 2.3082 - H0_pAt1: 0.6965 - H0_pAt5: 0.9327 - H1_pAt1: 0.6297 - H1_pAt5: 0.8600 - H2_pAt1: 0.5144 - H2_pAt5: 0.7563 - H3_pAt1: 0.4940 - H3_pAt5: 0.7462\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(x_tests,y_tests)\n",
    "dd = {k:v for k,v in zip(model.metrics_names,test_results)}\n",
    "dd['epoch']= 'evaluate'\n",
    "df = pd.read_csv(csv_log_dir)\n",
    "df = df.append(dd,ignore_index=True)\n",
    "df.to_csv(csv_log_dir,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>H0_loss</th>\n",
       "      <th>H0_pAt1</th>\n",
       "      <th>H0_pAt5</th>\n",
       "      <th>H1_loss</th>\n",
       "      <th>H1_pAt1</th>\n",
       "      <th>H1_pAt5</th>\n",
       "      <th>H2_loss</th>\n",
       "      <th>H2_pAt1</th>\n",
       "      <th>H2_pAt5</th>\n",
       "      <th>...</th>\n",
       "      <th>val_H1_loss</th>\n",
       "      <th>val_H1_pAt1</th>\n",
       "      <th>val_H1_pAt5</th>\n",
       "      <th>val_H2_loss</th>\n",
       "      <th>val_H2_pAt1</th>\n",
       "      <th>val_H2_pAt5</th>\n",
       "      <th>val_H3_loss</th>\n",
       "      <th>val_H3_pAt1</th>\n",
       "      <th>val_H3_pAt5</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.116257</td>\n",
       "      <td>0.676747</td>\n",
       "      <td>0.923616</td>\n",
       "      <td>1.587464</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.837904</td>\n",
       "      <td>2.405892</td>\n",
       "      <td>0.477661</td>\n",
       "      <td>0.716074</td>\n",
       "      <td>...</td>\n",
       "      <td>1.292744</td>\n",
       "      <td>0.65987</td>\n",
       "      <td>0.886177</td>\n",
       "      <td>1.952355</td>\n",
       "      <td>0.542961</td>\n",
       "      <td>0.791435</td>\n",
       "      <td>2.044484</td>\n",
       "      <td>0.522578</td>\n",
       "      <td>0.781094</td>\n",
       "      <td>6.206624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evaluate</td>\n",
       "      <td>1.054772</td>\n",
       "      <td>0.696482</td>\n",
       "      <td>0.932703</td>\n",
       "      <td>1.483419</td>\n",
       "      <td>0.629704</td>\n",
       "      <td>0.860027</td>\n",
       "      <td>2.210981</td>\n",
       "      <td>0.514402</td>\n",
       "      <td>0.756262</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch   H0_loss   H0_pAt1   H0_pAt5   H1_loss   H1_pAt1   H1_pAt5  \\\n",
       "0         0  1.116257  0.676747  0.923616  1.587464  0.606061  0.837904   \n",
       "1  evaluate  1.054772  0.696482  0.932703  1.483419  0.629704  0.860027   \n",
       "\n",
       "    H2_loss   H2_pAt1   H2_pAt5  ...  val_H1_loss  val_H1_pAt1  val_H1_pAt5  \\\n",
       "0  2.405892  0.477661  0.716074  ...     1.292744      0.65987     0.886177   \n",
       "1  2.210981  0.514402  0.756262  ...          NaN          NaN          NaN   \n",
       "\n",
       "   val_H2_loss  val_H2_pAt1  val_H2_pAt5  val_H3_loss  val_H3_pAt1  \\\n",
       "0     1.952355     0.542961     0.791435     2.044484     0.522578   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   val_H3_pAt5  val_loss  \n",
       "0     0.781094  6.206624  \n",
       "1          NaN       NaN  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(csv_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE PREDICTIONS TO : temp_bert/190812_153000_xmlcnn\n",
      "142756/142756 [==============================] - 11s 78us/sample\n"
     ]
    }
   ],
   "source": [
    "if args.save_weights:\n",
    "    model.save_weights(os.path.join(OUT_DIR,'weights.h5'))\n",
    "if args.save_model:\n",
    "    with open(os.path.join(OUT_DIR,'model.json'),'w') as f:\n",
    "        f.write(model.to_json())\n",
    "if args.save_prediction:\n",
    "    save_predictions(model,x_tests,y_tests,OUT_DIR)\n",
    "pd.DataFrame.from_dict([vars(args)]).to_csv(os.path.join(OUT_DIR,'args.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
