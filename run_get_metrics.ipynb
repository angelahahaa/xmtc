{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "from tools.model_func import get_input\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score,precision_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_k(y_true,y_pred,k,include_rank=False):\n",
    "    m,n = y_true.shape\n",
    "    rows = np.repeat(np.arange(m),k)\n",
    "    cols = y_pred[:,:k].flatten()\n",
    "    if include_rank:\n",
    "        data = np.tile(np.arange(k)+1,m)\n",
    "    else:\n",
    "        data = np.ones_like(rows)\n",
    "    return sp.csr_matrix((data,(rows,cols)),shape=(m,n))\n",
    "# categorical classification\n",
    "def get_top_k_accuracy(y_true,y_pred,k):\n",
    "    pred = get_sparse_k(y_true,y_pred,k)\n",
    "    return (y_true.multiply(pred).sum(axis=1)).A1\n",
    "# multi-label classification\n",
    "def get_nDCGAtk(y_true,y_pred,k):\n",
    "    pred = get_sparse_k(y_true,y_pred,k,include_rank=True)\n",
    "    pred.data = 1/np.log(pred.data+1)\n",
    "    dcg = y_true.multiply(pred).sum(axis=1).A1\n",
    "    num_labs = y_true[0,:].sum() # small cheat coz we know |y|_0 is constant\n",
    "    norm_const = (1/np.log(np.arange(min(k,num_labs))+2)).sum()\n",
    "    ndcg = dcg/norm_const\n",
    "    return ndcg\n",
    "def get_pAtk(y_true,y_pred,k):\n",
    "    pred = get_sparse_k(y_true,y_pred,k)\n",
    "    patk = (y_true.multiply(pred).sum(axis=1)/k).A1\n",
    "    return patk\n",
    "def get_micro_F1(y_true,y_pred,k):\n",
    "    pred = get_sparse_k(y_true,y_pred,k)\n",
    "    return f1_score(trues,pred,average='micro')\n",
    "def get_macro_precision(y_true,y_pred,k):\n",
    "    pred = get_sparse_k(y_true,y_pred,k)\n",
    "    return precision_score(trues,pred,average=None)\n",
    "def get_macro_F1(y_true,y_pred,k):\n",
    "    pred = get_sparse_k(y_true,y_pred,k)\n",
    "    return f1_score(trues,pred,average=None)\n",
    "# for HS\n",
    "def get_entire_H(y_true,y_pred,k):\n",
    "    # assume |y_pred|=the correct k\n",
    "    y_pred = np.sort(preds,axis=1)\n",
    "    row, _, _ = sp.find(trues.T)\n",
    "    y_true = row.reshape(y_pred.shape)\n",
    "    return np.all(y_true[:,:k]==y_pred[:,:k],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT HELPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(in_dir):\n",
    "    dirs = sorted([os.path.join(in_dir,d) for d in os.listdir(in_dir)])\n",
    "    out_d = defaultdict(list)\n",
    "    for d in dirs:\n",
    "        log_dir = os.path.join(d,'train.log')\n",
    "        args_dir = os.path.join(d,'args.csv')\n",
    "        if not os.path.exists(log_dir) or not os.path.exists(args_dir):\n",
    "            continue\n",
    "        df = pd.read_csv(log_dir)\n",
    "        arg = pd.read_csv(args_dir)\n",
    "        mode = arg.loc[0,'mode']\n",
    "        arg['dir'] = d\n",
    "        df['dir'] = d\n",
    "        out_d[mode].append(df)   \n",
    "        out_d['args'].append(arg)\n",
    "    args = pd.concat(out_d['args'], ignore_index = True, sort = False)\n",
    "    args = args[args['mode']=='cat']\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_by_probs(model_dir,offsets):\n",
    "    # need to change categorical ones to probs instead of logits later\n",
    "    out_dir = os.path.join(model_dir,'combined_pred_outputs.txt')\n",
    "    if os.path.exists(out_dir):\n",
    "        preds = np.loadtxt(out_dir)\n",
    "    else:\n",
    "        if 'FastText' in model_dir:\n",
    "            raise Exception('FastText predition does not exist: {}'.format(out_dir))\n",
    "        pred_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_outputs')])\n",
    "        preds = [np.loadtxt(pred_dir,dtype=int) for pred_dir in pred_dirs]\n",
    "        prob_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_probs')])\n",
    "        if not prob_dirs:\n",
    "            prob_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_logits')])\n",
    "        probs = [np.loadtxt(prob_dir) for prob_dir in prob_dirs]\n",
    "        for i in range(0,len(preds)):\n",
    "            preds[i]=preds[i]+offsets[i]\n",
    "        preds = np.concatenate(preds,axis=1)\n",
    "        # combined top k prediciton\n",
    "        probs = np.concatenate(probs,axis=1)\n",
    "        inds = np.argsort(probs,axis=1)[:,:-6:-1]\n",
    "        preds = np.take_along_axis(preds, inds, axis=1)\n",
    "        np.savetxt(out_dir,preds,fmt='%d')\n",
    "        print('SAVE COMBINED PREDICTIONS:\\n{}'.format(out_dir))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_per_H(model_dir,offsets):\n",
    "    out_dir = os.path.join(model_dir,'per_H_pred_outputs.txt')\n",
    "    if os.path.exists(out_dir):\n",
    "        preds = np.loadtxt(out_dir)\n",
    "    else:\n",
    "        pred_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_outputs')])\n",
    "        preds = [np.loadtxt(pred_dir,dtype=int,usecols=0)+offsets[i] for i,pred_dir in enumerate(pred_dirs)]\n",
    "        preds = np.vstack(preds).T\n",
    "        prob_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_probs')])\n",
    "        if not prob_dirs:\n",
    "            prob_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_logits')])\n",
    "        probs = [np.loadtxt(prob_dir,usecols=0) for prob_dir in prob_dirs]\n",
    "        probs = np.vstack(probs).T\n",
    "        inds = np.argsort(probs,axis=1)[:,::-1]\n",
    "        preds = np.take_along_axis(preds, inds, axis=1)\n",
    "        np.savetxt(out_dir,preds,fmt='%d')\n",
    "        print('SAVE PER H PREDICTIONS:\\n{}'.format(out_dir))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model_dir,y_tests,mode):\n",
    "    cnts = [y_tests[i].shape[1] for i in range(len(y_tests))]\n",
    "    offsets = [0] + [cnts[i]+sum(cnts[:i]) for i in range(len(cnts))]\n",
    "    if mode == 'top_probs':\n",
    "        preds = order_by_probs(model_dir,offsets)\n",
    "    elif mode == 'top_per_H':\n",
    "        preds = order_per_H(model_dir,offsets)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_preds(model_dir,topk=5):\n",
    "    pred_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_outputs')])\n",
    "    preds = [np.loadtxt(pred_dir,dtype=int,usecols=np.arange(topk)).reshape([-1,topk]) for pred_dir in pred_dirs]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get groups\n",
    "def get_groups(y_trains,num_groups = 3):\n",
    "    # get train label frequencey\n",
    "    train_cnts = np.hstack([y.sum(axis=0).A1 for y in y_trains])\n",
    "    lab_to_cnts = {i:cnt for i,cnt in enumerate(train_cnts)}\n",
    "    sorted_labs = sorted(lab_to_cnts.keys(),key=lambda x:lab_to_cnts[x])\n",
    "    # get groups by count\n",
    "    group_cnt = sum([cnt for cnt in lab_to_cnts.values()])/num_groups\n",
    "    groups = []\n",
    "    accumulated_cnts = 0\n",
    "    group = []\n",
    "    for lab in sorted_labs:\n",
    "        accumulated_cnts+=lab_to_cnts[lab]\n",
    "        if accumulated_cnts>group_cnt and len(groups)<num_groups:\n",
    "            groups.append(group)\n",
    "            group = [lab]\n",
    "            accumulated_cnts = 0\n",
    "        else:\n",
    "            group.append(lab)\n",
    "    groups.append(group)\n",
    "    # print things\n",
    "    cut_off = [0]+[lab_to_cnts[g[-1]] for g in groups]\n",
    "    t_bound = ['${} < t < {}$'.format(cut_off[i],cut_off[i+1])for i in range(len(groups))]\n",
    "    lab_per_group = [len(g) for g in groups]\n",
    "    perc_lab_per_group = [g/sum(lab_per_group)*100 for g in lab_per_group]\n",
    "    df = pd.DataFrame()\n",
    "    df['group'] = ['G{}'.format(g) for g in range(len(groups))]\n",
    "    df['num_train'] = [sum([lab_to_cnts[lab] for lab in group])for group in groups]\n",
    "    df['perc_train'] = df['num_train']/df['num_train'].sum()*100\n",
    "    \n",
    "    df['num_train_cut_off'] = cut_off[1:]\n",
    "    df['t_bound'] = t_bound\n",
    "    df['num_labels'] = lab_per_group\n",
    "    df['perc_labels'] = df['num_labels']/df['num_labels'].sum()*100\n",
    "    return groups,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not care about the std, only micro averaging is used!! :D\n",
    "def get_categorical_scores(d,y_tests,metrics,ks):\n",
    "    preds = get_categorical_preds(d,max(ks))\n",
    "    df = pd.DataFrame()\n",
    "    for H in range(len(preds)):\n",
    "        df.loc[H,'H']='H{}'.format(H)\n",
    "        for key,func in metrics.items():\n",
    "            for k in ks:\n",
    "                metric = key.format(k)\n",
    "                df.loc[H,metric] = func(y_tests[H],preds[H],k).mean() # func returns metric per sample\n",
    "    df['dir'] = d\n",
    "    df['model'] = d.split('_')[-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_micro_scores(d,trues,y_tests,metrics,ks,mode):\n",
    "    preds = get_preds(d,y_tests,mode)\n",
    "    dic = {}\n",
    "    dic['dir'] = d\n",
    "    dic['model'] = d.split('_')[-1]\n",
    "    for key,func in metrics.items():\n",
    "        for k in ks:\n",
    "            metric = key.format(k)\n",
    "            if metric in dic.keys():\n",
    "                continue\n",
    "            dic[metric] = func(trues,preds,k).mean()\n",
    "    return pd.DataFrame.from_dict([dic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the std here :]\n",
    "def get_macro_scores(d,trues,y_tests,metrics,ks,mode,groups):\n",
    "    preds = get_preds(d,y_tests,mode)\n",
    "    df = pd.DataFrame(index = [ind for group in groups for ind in group])\n",
    "    df.index.name = 'lab_ind'\n",
    "    for G,group in enumerate(groups):\n",
    "        df.loc[group,'G']='G{}'.format(G)\n",
    "    for key,func in metrics.items():\n",
    "        for k in ks:\n",
    "            ss = func(trues,preds,k)\n",
    "            for group in groups:\n",
    "                df.loc[group,key.format(k)] = ss[group]\n",
    "    df['dir'] = d\n",
    "    df['model'] = d.split('_')[-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ENTIRE H CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args('outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "dff = pd.DataFrame()\n",
    "df = args\n",
    "df = df[df['mode']=='cat']\n",
    "df = df.set_index(['input','loss']).sort_index()\n",
    "df_ind = 0\n",
    "for i,data in enumerate(['sic_hierarchy','amazon_hierarchy_2']):\n",
    "    if i==1:\n",
    "        break\n",
    "    in_dir = 'data/{}'.format(data)\n",
    "    _,y_trains,_,y_tests = get_input(mode='cat', in_dir = in_dir, sparse = True, get_output= [0,1,0,1])\n",
    "    trues = sp.hstack(y_tests).tocsr()\n",
    "    groups,_ = get_groups(y_trains,num_groups = 3)\n",
    "    for loss in ['binary','categorical','masked_categorical']:\n",
    "        dirs = sorted(df.loc[(in_dir,loss)].dir.to_list())\n",
    "        for d in dirs:\n",
    "            dff.loc[df_ind,'dir']=d\n",
    "            dff.loc[df_ind,'loss']=loss\n",
    "            dff.loc[df_ind,'model']=d.split('_')[-1].split('.')[0]\n",
    "            preds = get_preds(d,y_tests,'top_probs')[:,:len(y_tests)]\n",
    "            for H in range(preds.shape[1]):\n",
    "                score = get_entire_H(trues,preds,H+1).mean()\n",
    "                dff.loc[df_ind,'H{}'.format(H)]=score\n",
    "            df_ind+=1\n",
    "            print('.',end='')\n",
    "# df = pd.concat(dfs)\n",
    "# df.to_pickle('outputs/dfs/macro.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dff\n",
    "# df2 = df2[df2['model']!='bert']\n",
    "df2 = df2[df2['dir']!='outputs/190810_151748_bert'] # this bert does worst\n",
    "df2 = df2.set_index(['loss','model']).drop(columns=['dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df2 - df2.loc['binary'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICRO SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args('outputs')\n",
    "warnings.filterwarnings('ignore')\n",
    "kss = [[1,3,5,4],[1,3,5]]\n",
    "metrics = {\n",
    "    'P@{}':get_pAtk,\n",
    "    'nDCG@{}':get_nDCGAtk,\n",
    "    'F1':lambda y_true,y_pred,k:get_micro_F1(y_true,y_pred,1)\n",
    "}\n",
    "datas = ['sic_hierarchy','amazon_hierarchy_2']\n",
    "loss_mode = [\n",
    "    ('binary','top_probs'),\n",
    "    ('categorical','top_probs'),\n",
    "    ('masked_categorical','top_probs'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start\n",
    "dfs=[]\n",
    "df = args\n",
    "df = df[df['mode']=='cat']\n",
    "df = df.set_index(['input','loss']).sort_index()\n",
    "df_ind = 0\n",
    "for i,data in enumerate(datas):\n",
    "    in_dir = 'data/{}'.format(data)\n",
    "    ks = kss[i]\n",
    "    _,y_trains,_,y_tests = get_input(mode='cat', in_dir = in_dir, sparse = True, get_output= [0,1,0,1])\n",
    "    trues = sp.hstack(y_tests).tocsr()\n",
    "    groups,_ = get_groups(y_trains,num_groups = 3)\n",
    "    for loss,mode in loss_mode:\n",
    "        if (in_dir,loss) not in list(df.index.values):\n",
    "            continue\n",
    "        else:\n",
    "            print(data,loss,mode)\n",
    "        dirs = sorted(df.loc[(in_dir,loss)].dir.to_list())\n",
    "        for d in dirs:\n",
    "            df1 = get_micro_scores(d,trues,y_tests,metrics,ks,mode)\n",
    "            df1['loss']=loss\n",
    "            df1['data']=data\n",
    "            df1['mode']=mode\n",
    "            dfs.append(df1)\n",
    "            print('.',end='')\n",
    "        print()\n",
    "df = pd.concat(dfs)\n",
    "# df.to_pickle('outputs/dfs/micro.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('outputs/dfs/micro.pkl')\n",
    "models = ['xmlcnn','attentionxml','attention','bert']\n",
    "indexs = ['loss','data','model']\n",
    "display_metrics = ['F1','P@1','P@3','P@4','nDCG@1','nDCG@3','nDCG@5']\n",
    "\n",
    "df2 = df\n",
    "df2 = df2[df2['dir']!='outputs/190810_151748_bert'] # this bert does worst\n",
    "df2 = df2[df2['model'].isin(models)]\n",
    "# df2['loss'] = df2['loss']+','+df2['mode']\n",
    "df2 = df2[indexs+display_metrics]\n",
    "df2 = df2.melt(\n",
    "    id_vars = indexs,\n",
    "    value_vars = display_metrics,\n",
    "    var_name = 'metric',\n",
    "    value_name = 'score',\n",
    ")\n",
    "df2 = df2.set_index(['loss','data','metric','model'])\n",
    "df2 = (df2.reindex(datas,level=1).reindex(models,level=-1).unstack([1,-1])*100)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for latex\n",
    "df3 = df2.stack(-2).reset_index().set_index(['loss','data','metric']).sort_index().droplevel(0,1).reindex(datas,level=1)[models]\n",
    "df3.index.names = [None] * len(df3.index.names)\n",
    "print(df3.loc['categorical'].to_latex(float_format='%.2f').replace(' ','').replace('&',' & '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _color_red_or_green(val):\n",
    "    c='black'\n",
    "    if val <= -1:\n",
    "        c = 'red'\n",
    "    elif val >= 1:\n",
    "        c = 'green'\n",
    "    return 'color: %s' % c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df2.loc['categorical']-df2.loc['binary']).style.applymap(_color_red_or_green).format('{:.2f}')\n",
    "# df2.map(lambda x:'{:.2f}'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args('outputs')\n",
    "warnings.filterwarnings('ignore')\n",
    "kss = [[1],[1]]\n",
    "metrics = {\n",
    "    'P@{}':get_pAtk,\n",
    "}\n",
    "dfs=[]\n",
    "df = args\n",
    "df = df[df['mode']=='cat']\n",
    "df = df.set_index(['input','loss']).sort_index()\n",
    "df_ind = 0\n",
    "for i,data in enumerate(['sic_hierarchy','amazon_hierarchy_2']):\n",
    "    if i==1:\n",
    "        break\n",
    "    in_dir = 'data/{}'.format(data)\n",
    "    ks = kss[i]\n",
    "    _,y_trains,_,y_tests = get_input(mode='cat', in_dir = in_dir, sparse = True, get_output= [0,1,0,1])\n",
    "    for loss in ['categorical','masked_categorical']:\n",
    "        dirs = sorted(df.loc[(in_dir,loss)].dir.to_list())\n",
    "        for d in dirs:\n",
    "            df1 = get_categorical_scores(d,y_tests,metrics,ks)\n",
    "            df1['loss']=loss\n",
    "            df1['input']=in_dir\n",
    "            df1['mode']=mode\n",
    "            dfs.append(df1)\n",
    "            print('.',end='')\n",
    "# df = pd.concat(dfs)\n",
    "# df.to_pickle('outputs/dfs/macro.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)\n",
    "df2 = df[df['model']!='bert'].set_index(['loss','model','H']).drop(columns=['dir','mode','input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.unstack(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((df2.loc['masked_categorical'] - df2.loc['categorical'])*100).unstack(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACRO SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args('outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "kss = [[4],[3]]\n",
    "metrics = {\n",
    "    'precision':get_macro_precision,\n",
    "    'F1':get_macro_F1\n",
    "}\n",
    "datas = ['sic_hierarchy','amazon_hierarchy_2']\n",
    "loss_mode = [\n",
    "    ('binary','top_probs'),\n",
    "    ('categorical','top_probs'),\n",
    "    ('masked_categorical','top_probs'),\n",
    "]\n",
    "# function\n",
    "dfs = []\n",
    "df = args\n",
    "df = df[df['mode']=='cat']\n",
    "df = df.set_index(['input','loss']).sort_index()\n",
    "for i,data in enumerate(datas):\n",
    "    in_dir = 'data/{}'.format(data)\n",
    "    ks = kss[i]\n",
    "    _,y_trains,_,y_tests = get_input(mode='cat', in_dir = in_dir, sparse = True, get_output= [0,1,0,1])\n",
    "    trues = sp.hstack(y_tests).tocsr()\n",
    "    groups,_ = get_groups(y_trains,num_groups = 3)\n",
    "    for loss,mode in loss_mode:\n",
    "        index = (in_dir,loss)\n",
    "        if (in_dir,loss) not in list(df.index.values):\n",
    "            continue\n",
    "        else:\n",
    "            print(data,loss,mode)\n",
    "        dirs = sorted(df.loc[(in_dir,loss)].dir.to_list())\n",
    "        if loss == 'binary':\n",
    "            dirs = ['outputs/{}_c_FastText'.format(data)] + dirs\n",
    "        for d in dirs:\n",
    "            print('.',end='')\n",
    "            df1 = get_macro_scores(d,trues,y_tests,metrics,ks,mode,groups)\n",
    "            df1['loss']=loss\n",
    "            df1['input']=in_dir\n",
    "            df1['mode']=mode\n",
    "            dfs.append(df1)\n",
    "        print()\n",
    "df = pd.concat(dfs)\n",
    "df.to_pickle('outputs/dfs/macro.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TABLE : macro scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('outputs/dfs/macro.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['FastText','xmlcnn','attentionxml','attention','bert']\n",
    "indexs = ['loss','input','model']\n",
    "display_metrics = ['precision','F1']\n",
    "datas = ['data/sic_hierarchy','data/amazon_hierarchy_2']\n",
    "df2 = df\n",
    "df2 = df2[df2['dir']!='outputs/190810_151748_bert'] # this bert does worst\n",
    "df2 = df2.drop(columns=['dir','mode']).groupby(indexs).mean().reset_index()\n",
    "df2 = df2.melt(    \n",
    "    id_vars = indexs,\n",
    "    value_vars = display_metrics,\n",
    "    var_name = 'metric',\n",
    "    value_name = 'score')\n",
    "df2 = df2.set_index(indexs+['metric'])\n",
    "df2 = df2.reindex(datas,level=1).reindex(models,level=2).reindex(display_metrics,level=-1)\n",
    "df2 = df2.unstack(-2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((df2.loc['categorical']-df2.loc['binary'])*100).style.applymap(_color_red_or_green).format('{:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE\n",
    "datas = ['data/sic_hierarchy','data/amazon_hierarchy_2']\n",
    "models = ['FastText','xmlcnn','attentionxml','attention']\n",
    "metrics = ['precision','F1']\n",
    "for data in datas:\n",
    "    print(data)\n",
    "    df2 = df1[df1.input==data]\n",
    "    srs = (df2.groupby('model')[metrics].mean()*100).to_dict()\n",
    "    for metric in metrics:\n",
    "        print('{:10}'.format(metric),end=':')\n",
    "        print('&'.join(['{:.2f}'.format(srs[metric][key]) for key in models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((df2.loc['categorical'])*100).to_latex(float_format='\\tg{%.2f}',escape=False).replace(' ','').replace('&',' & '))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT : macro scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('outputs/dfs/macro.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {\n",
    "    'precision':'Precision',\n",
    "    'F1':'F1 score',\n",
    "}\n",
    "data_dict = {\n",
    "    'data/sic_hierarchy':'SIC Code',\n",
    "    'data/amazon_hierarchy_2':'AmazonCat-13k',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance of binary loss\n",
    "datas = ['data/sic_hierarchy','data/amazon_hierarchy_2']\n",
    "models = ['FastText','xmlcnn','attentionxml','attention']\n",
    "metrics = ['precision','F1']\n",
    "# function\n",
    "df1 = df.reset_index()\n",
    "df1 = df1[df1['model'].isin(models)]\n",
    "df1 = df1[df1['loss']=='binary']\n",
    "for data in datas:\n",
    "    df2 = df1[df1.input==data]\n",
    "    for metric in metrics:\n",
    "        # y tick counts\n",
    "        cnts = (df2['G'].value_counts()/df2['G'].value_counts().sum()*100).to_dict()\n",
    "        groups = sorted(cnts.keys())\n",
    "        # plot\n",
    "        fig,ax = plt.subplots()\n",
    "        bar = sns.barplot(\n",
    "            x = 'G',\n",
    "            y=metric,\n",
    "            hue='model',\n",
    "            data = df2,\n",
    "            ax = ax, \n",
    "            palette=sns.color_palette(\"Set3\"),\n",
    "            order=groups,\n",
    "            hue_order = models,\n",
    "            edgecolor = 'k',\n",
    "            linewidth=1,\n",
    "            ci=None\n",
    "           )\n",
    "        ax.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "               ncol=len(models), mode=\"expand\", borderaxespad=0.)\n",
    "        ax.set_ylabel(metric_dict[metric])\n",
    "        ax.set_xlabel('{} label groups (% labels in group)'.format(data_dict[data]))\n",
    "        ax.set_xticklabels(['{} ({:.2f}%)'.format(key,cnts[key]) for key in groups])\n",
    "        ax.set_ylim(0,1)\n",
    "#         ax.axhline(y=0, color='k')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance of categorical loss (bin as ref)\n",
    "# PARAMS\n",
    "datas = ['data/sic_hierarchy','data/amazon_hierarchy_2']\n",
    "models = ['xmlcnn','attentionxml','attention']\n",
    "metrics = ['precision','F1']\n",
    "# function\n",
    "df1 = df.reset_index()\n",
    "df1 = df1[df1['model'].isin(models)]\n",
    "df1 = df1.set_index(['loss','input','model','G','lab_ind'])\n",
    "df1 = df1.drop(columns=['dir','mode'])\n",
    "df1b = (df1.loc['binary']).reset_index()\n",
    "df1 = (df1.loc['categorical']).reset_index()\n",
    "# fig,axes = plt.subplots(2,2,sharey=True,figsize=(10,8))\n",
    "for j,data in enumerate(datas):\n",
    "    df2 = df1[df1.input==data]\n",
    "    for i,metric in enumerate(metrics):\n",
    "        fig,ax = plt.subplots()\n",
    "#         ax = axes[i,j]\n",
    "        # y tick counts\n",
    "        cnts = (df2['G'].value_counts()/df2['G'].value_counts().sum()*100).to_dict()\n",
    "        groups = sorted(cnts.keys())\n",
    "        # plot\n",
    "        bar1 = sns.barplot(\n",
    "            x = 'G',\n",
    "            y=metric,\n",
    "            hue='model',\n",
    "            data = df2,\n",
    "            ax = ax, \n",
    "            palette=sns.color_palette(\"Set3\")[1:],\n",
    "            order=groups,\n",
    "            hue_order = models,\n",
    "            ci=None,\n",
    "            edgecolor = 'k',\n",
    "            linewidth=1,\n",
    "           )\n",
    "        bar2 = sns.barplot(\n",
    "            x = 'G',\n",
    "            y=metric,\n",
    "            hue='model',\n",
    "            data = df1b[df1b.input==data],\n",
    "            ax = ax, \n",
    "            palette=sns.color_palette(\"Reds\")[1:],\n",
    "            order=groups,\n",
    "            hue_order = models,\n",
    "            ci=None,\n",
    "            fill=False,\n",
    "            alpha=0.5,\n",
    "            edgecolor = 'k',\n",
    "            linewidth=1,\n",
    "            linestyle='--',\n",
    "           )\n",
    "        ax.legend(models,bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "               ncol=len(models), mode=\"expand\", borderaxespad=0.)\n",
    "        ax.set_ylabel('{}'.format(metric_dict[metric]))\n",
    "        ax.set_xlabel('{} label groups (% labels in group)'.format(data_dict[data]))\n",
    "        ax.set_xticklabels(['{} ({:.2f}%)'.format(key,cnts[key]) for key in groups])\n",
    "        ax.axhline(y=0, color='k')\n",
    "        ax.set_ylim(0,1)\n",
    "        plt.show()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance of HS (bin as ref)\n",
    "# PARAMS\n",
    "datas = ['data/sic_hierarchy','data/amazon_hierarchy_2']\n",
    "models = ['xmlcnn','attentionxml','attention']\n",
    "metrics = ['precision','F1']\n",
    "# function\n",
    "df1 = df.reset_index()\n",
    "df1 = df1[df1['model'].isin(models)]\n",
    "df1 = df1.set_index(['loss','input','model','G','lab_ind'])\n",
    "df1 = df1.drop(columns=['dir','mode'])\n",
    "df1b = (df1.loc['binary']).reset_index()\n",
    "df1 = (df1.loc['masked_categorical']).reset_index()\n",
    "fig,axes = plt.subplots(2,2,sharey=True,figsize=(10,8))\n",
    "for j,data in enumerate(datas):\n",
    "    df2 = df1[df1.input==data]\n",
    "    if j==1:\n",
    "        continue\n",
    "    for i,metric in enumerate(metrics):\n",
    "#         fig,ax = plt.subplots()\n",
    "        ax = axes[i,j]\n",
    "        # y tick counts\n",
    "        cnts = (df2['G'].value_counts()/df2['G'].value_counts().sum()*100).to_dict()\n",
    "        groups = sorted(cnts.keys())\n",
    "        # plot\n",
    "        bar1 = sns.barplot(\n",
    "            x = 'G',\n",
    "            y=metric,\n",
    "            hue='model',\n",
    "            data = df2,\n",
    "            ax = ax, \n",
    "            palette=sns.color_palette(\"Set3\")[1:],\n",
    "            order=groups,\n",
    "            hue_order = models,\n",
    "            ci=None,\n",
    "            edgecolor = 'k',\n",
    "            linewidth=1,\n",
    "           )\n",
    "        bar2 = sns.barplot(\n",
    "            x = 'G',\n",
    "            y=metric,\n",
    "            hue='model',\n",
    "            data = df1b[df1b.input==data],\n",
    "            ax = ax, \n",
    "            palette=sns.color_palette(\"Reds\")[1:],\n",
    "            order=groups,\n",
    "            hue_order = models,\n",
    "            ci=None,\n",
    "            fill=False,\n",
    "            alpha=0.5,\n",
    "            edgecolor = 'k',\n",
    "            linewidth=1,\n",
    "            linestyle='--',\n",
    "           )\n",
    "        ax.legend(models,bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "               ncol=len(models), mode=\"expand\", borderaxespad=0.)\n",
    "        ax.set_ylabel('{}'.format(metric_dict[metric]))\n",
    "        ax.set_xlabel('{} label groups (% labels in group)'.format(data_dict[data]))\n",
    "        ax.set_xticklabels(['{} ({:.2f}%)'.format(key,cnts[key]) for key in groups])\n",
    "        ax.set_ylim(0,1)\n",
    "#         plt.show()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference, cat - bin\n",
    "# PARAMS\n",
    "datas = ['data/sic_hierarchy','data/amazon_hierarchy_2']\n",
    "models = ['xmlcnn','attentionxml','attention']\n",
    "metrics = ['precision','F1']\n",
    "# function\n",
    "df1 = df.reset_index()\n",
    "df1 = df1[df1['model'].isin(models)]\n",
    "df1 = df1.set_index(['loss','input','model','G','lab_ind'])\n",
    "df1 = df1.drop(columns=['dir','mode'])\n",
    "df1 = (df1.loc['categorical']-df1.loc['binary']).reset_index()\n",
    "fig,axes = plt.subplots(2,2,sharey=True,figsize=(10,8))\n",
    "for j,data in enumerate(datas):\n",
    "    df2 = df1[df1.input==data]\n",
    "    for i,metric in enumerate(metrics):\n",
    "#         fig,ax = plt.subplots()\n",
    "        ax = axes[i,j]\n",
    "        # y tick counts\n",
    "        cnts = (df2['G'].value_counts()/df2['G'].value_counts().sum()*100).to_dict()\n",
    "        groups = sorted(cnts.keys())\n",
    "        # plot\n",
    "        bar = sns.barplot(\n",
    "            x = 'G',\n",
    "            y=metric,\n",
    "            hue='model',\n",
    "            data = df2,\n",
    "            ax = ax, \n",
    "            palette=sns.color_palette(\"Set3\")[1:],\n",
    "            order=groups,\n",
    "            hue_order = models,\n",
    "            edgecolor = 'k',\n",
    "            linewidth=1,\n",
    "            ci=None\n",
    "           )\n",
    "        ax.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "               ncol=len(models), mode=\"expand\", borderaxespad=0.)\n",
    "        ax.set_ylabel('$\\Delta$ {}'.format(metric_dict[metric]))\n",
    "        ax.set_xlabel('{} label groups (% labels in group)'.format(data_dict[data]))\n",
    "        ax.set_xticklabels(['{} ({:.2f}%)'.format(key,cnts[key]) for key in groups])\n",
    "        ax.axhline(y=0, color='k',linewidth=1)\n",
    "        ax.set_ylim(-0.02,0.09)\n",
    "#         plt.show()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference, HS - bin\n",
    "# PARAMS\n",
    "datas = ['data/sic_hierarchy','data/amazon_hierarchy_2']\n",
    "models = ['xmlcnn','attentionxml','attention']\n",
    "metrics = ['precision','F1']\n",
    "# function\n",
    "df1 = df.reset_index()\n",
    "df1 = df1[df1['model'].isin(models)]\n",
    "df1 = df1.set_index(['loss','input','model','G','lab_ind'])\n",
    "df1 = df1.drop(columns=['dir','mode'])\n",
    "df1 = (df1.loc['masked_categorical']-df1.loc['binary']).reset_index()\n",
    "fig,axes = plt.subplots(2,2,sharey=True,figsize=(10,8))\n",
    "for j,data in enumerate(datas):\n",
    "    df2 = df1[df1.input==data]\n",
    "    for i,metric in enumerate(metrics):\n",
    "#         fig,ax = plt.subplots()\n",
    "        ax = axes[i,j]\n",
    "        # y tick counts\n",
    "        cnts = (df2['G'].value_counts()/df2['G'].value_counts().sum()*100).to_dict()\n",
    "        groups = sorted(cnts.keys())\n",
    "        # plot\n",
    "        bar = sns.barplot(\n",
    "            x = 'G',\n",
    "            y=metric,\n",
    "            hue='model',\n",
    "            data = df2,\n",
    "            ax = ax, \n",
    "            palette=sns.color_palette(\"Set3\")[1:],\n",
    "            order=groups,\n",
    "            hue_order = models,\n",
    "            edgecolor = 'k',\n",
    "            linewidth=1,\n",
    "            ci=None\n",
    "           )\n",
    "        ax.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "               ncol=len(models), mode=\"expand\", borderaxespad=0.)\n",
    "        ax.set_ylabel('$\\Delta$ {}'.format(metric_dict[metric]))\n",
    "        ax.set_xlabel('{} label groups (% labels in group)'.format(data_dict[data]))\n",
    "        ax.set_xticklabels(['{} ({:.2f}%)'.format(key,cnts[key]) for key in groups])\n",
    "        ax.axhline(y=0, color='k',linewidth=1)\n",
    "        ax.set_ylim(-0.02,0.09)\n",
    "#         plt.show()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
