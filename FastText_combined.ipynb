{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0829 15:01:40.504022 140214518523712 deprecation_wrapper.py:119] From /home/angela/xmtc/tools/model_func.py:21: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os,datetime,re\n",
    "import fasttext\n",
    "from tools.model_func import get_input\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_SIZE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create FT data from sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ft(out_dir,xs,ys):\n",
    "    s = xs[0].shape[0]\n",
    "    with open(out_dir,'w') as f:\n",
    "        for i in range(s):\n",
    "            f.write(' '.join(['__label__{}_{}'.format(j,y[i]) for j,y in enumerate(ys)]))\n",
    "            f.write(' ')\n",
    "            f.write(' '.join(np.trim_zeros(xs[0][i,:],'b').astype(str)))\n",
    "            f.write('\\n')\n",
    "            if i%(s//100)==0:\n",
    "                print('{:.2f}%'.format(i/s*100),end='\\r')\n",
    "    print('saved to: {}'.format(out_dir))\n",
    "def create_FT_data(in_dir,mode):\n",
    "    name = in_dir.split('/')[-1]\n",
    "    out_dir = 'data/FT/{}_{}'.format(name,mode[0])\n",
    "    x_trains,y_trains,x_tests,y_tests = get_input(in_dir = in_dir, mode = mode,get_output=[1,1,1,1],sparse = True)\n",
    "    y_trains = [y.argmax(axis=1).A1 for y in y_trains]\n",
    "    y_tests = [y.argmax(axis=1).A1 for y in y_tests]\n",
    "    print('TRAIN')\n",
    "    make_ft(out_dir + '.train.txt',x_trains,y_trains)\n",
    "    print('TEST')\n",
    "    make_ft(out_dir + '.test.txt',x_tests,y_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_FT_data('data/sic_hierarchy','hierarchy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train FastText model from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fastText(in_dir,out_dir = 'FastText/models',lr=None,loss='ova',epoch=5,save_model = True,save_pred = True):\n",
    "    if loss not in ['ova','hs']:\n",
    "        raise Exception('Unkown loss: {}'.format(loss))\n",
    "    data = in_dir.split('/')[-1].split('.')[0]\n",
    "    model_dir= os.path.join(\n",
    "        out_dir,\n",
    "        datetime.datetime.now().strftime('%y%m%d_%H%M%S_{}_{}.bin'.format(data,loss)),\n",
    "    )\n",
    "    if lr is None:\n",
    "        if loss == 'ova':\n",
    "            lr = 0.1\n",
    "        elif loss == 'hs':\n",
    "            lr = 1.0\n",
    "    model = fasttext.train_supervised(\n",
    "        input=in_dir,\n",
    "        epoch=epoch,\n",
    "        lr=lr,\n",
    "        wordNgrams=2,\n",
    "        minCount=1,\n",
    "        loss = loss,\n",
    "        )\n",
    "    if save_model:\n",
    "        model.save_model(model_dir)\n",
    "        print('Model saved to:\\n{}'.format(model_dir))\n",
    "    return model,model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model,model_dir = run_fastText('data/FT/amazon_hierarchy_2_c.train.txt',loss='ova')\n",
    "# model,model_dir = run_fastText('data/FT/sic_hierarchy_c.train.txt',loss='ova')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test FastText models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "def get_test_data(test_dir):\n",
    "    label_pattern = re.compile('__label__\\S+')\n",
    "    true_labels = []\n",
    "    contents = []\n",
    "    with open(test_dir,'r',encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f.read().splitlines():\n",
    "            true_labels.append(set(label_pattern.findall(line)))\n",
    "            contents.append(label_pattern.sub(r'',line).strip())\n",
    "    return true_labels,contents\n",
    "def get_prediction(model_dir, contents, top_k = PRED_SIZE, save_preds = True, save_probs= False):\n",
    "    model = fasttext.load_model(model_dir)\n",
    "    preds = []\n",
    "    probs = []\n",
    "    s = len(contents)\n",
    "    for i in range(s):\n",
    "        p,l = model.predict(contents[i],k=top_k)\n",
    "        preds.append(p)\n",
    "        probs.append(l)\n",
    "        if i%(s//100)==0:\n",
    "            print('{:.2f}%'.format(i/s*100),end='\\r')\n",
    "    PRED_DIR = model_dir.split('.')[0]+'_pred_outputs.pkl'\n",
    "    PROB_DIR = model_dir.split('.')[0]+'_pred_probs.pkl'\n",
    "    if save_preds:\n",
    "        with open(PRED_DIR, 'wb') as f:\n",
    "            pickle.dump(preds, f)\n",
    "        print('SAVE PREDICTION TO:\\n{}'.format(PRED_DIR))\n",
    "    if save_probs:\n",
    "        with open(PROB_DIR, 'wb') as f:\n",
    "            pickle.dump(probs, f)\n",
    "        print('SAVE PROBABILITIES TO:\\n{}'.format(LOG_DIR))\n",
    "    return preds,probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct(true_labels,raw_preds,match=None):\n",
    "    # turn raw labels to multilabel metrics\n",
    "    s = len(true_labels)\n",
    "    outputs = np.zeros(shape=(s,PRED_SIZE))\n",
    "    for i in range(s):\n",
    "        k=0\n",
    "        for pred in raw_preds[i]:\n",
    "            if match is not None and match not in pred:\n",
    "                continue\n",
    "            if pred in true_labels[i]:\n",
    "                outputs[i,k]=1\n",
    "            k+=1\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all labels\n",
    "def get_all_labels(true_labels):\n",
    "    labs = set()\n",
    "    for lab in true_labels:\n",
    "        for l in lab:\n",
    "            ss = l.split('_')\n",
    "            labs.add((int(ss[-2]),int(ss[-1])))\n",
    "    return labs\n",
    "def get_lab_to_ind_dict(labs,per_hierarchy):\n",
    "    lab_to_ind_dict = {}\n",
    "    if not per_hierarchy:\n",
    "        # get offset\n",
    "        a, _ =zip(*list(labs))\n",
    "        a = np.array(a)\n",
    "        cnts = [np.sum(a==i)for i in range(max(a))]\n",
    "        offset = [0] + [sum(cnts[:i+1]) for i in range(len(cnts))]\n",
    "        # get dict\n",
    "        for lab in labs:\n",
    "            lab_to_ind_dict['__label__{}_{}'.format(lab[0],lab[1])] = lab[1]+offset[lab[0]]\n",
    "    else:\n",
    "         for lab in labs:\n",
    "            lab_to_ind_dict['__label__{}_{}'.format(lab[0],lab[1])] = lab\n",
    "    return lab_to_ind_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print existing models\n",
    "print('Existing Test Data:\\n')\n",
    "dd = 'data/FT'\n",
    "md = 'FastText/models'\n",
    "data_dirs = [os.path.join(dd,d) for d in os.listdir(dd) if 'test' in d]\n",
    "model_dirs = [os.path.join(md,d) for d in os.listdir(md) if 'bin' in d]\n",
    "for data_dir in data_dirs:\n",
    "    for model_dir in model_dirs:\n",
    "        if data_dir.split('/')[-1].split('.')[0] in model_dir:\n",
    "            print('TEST_DIR = \\'{}\\''.format(data_dir))\n",
    "            print('MODEL_DIR = \\'{}\\''.format(model_dir))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = 'data/FT/sic_hierarchy_c.test.txt'\n",
    "MODEL_DIR = 'FastText/models/190823_195436_sic_hierarchy_c_ova.bin'\n",
    "true_labels,contents = get_test_data(TEST_DIR)\n",
    "raw_preds,_ = get_prediction(MODEL_DIR,contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make FT data like DL data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print existing models\n",
    "print('Existing Test Data:\\n')\n",
    "dd = 'data/FT'\n",
    "md = 'FastText/models'\n",
    "data_dirs = [os.path.join(dd,d) for d in os.listdir(dd) if 'test' in d]\n",
    "model_dirs = [os.path.join(md,d) for d in os.listdir(md) if 'pred' in d]\n",
    "for data_dir in data_dirs:\n",
    "    for model_dir in model_dirs:\n",
    "        if data_dir.split('/')[-1].split('.')[0] in model_dir:\n",
    "            print('TEST_DIR = \\'{}\\''.format(data_dir))\n",
    "            print('PRED_DIR = \\'{}\\''.format(model_dir))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = 'data/FT/sic_hierarchy_c.test.txt'\n",
    "PRED_DIR = 'FastText/models/190823_195436_sic_hierarchy_c_ova_pred_outputs.pkl'\n",
    "true_labels,contents = get_test_data(TEST_DIR)\n",
    "raw_preds = pickle.load(open(PRED_DIR,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all labels\n",
    "labs = get_all_labels(true_labels)\n",
    "lab_to_ind_dict = get_lab_to_ind_dict(labs,per_hierarchy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert raw_preds to numbers\n",
    "s = len(raw_preds)\n",
    "k = 5\n",
    "preds = np.ones(shape = (s,k))*-1\n",
    "for i in range(s):\n",
    "    for j,lab in enumerate(raw_preds[i]):\n",
    "        if j>=k:\n",
    "            break\n",
    "        preds[i,j]=lab_to_ind_dict[lab]\n",
    "# check no empty\n",
    "print('missing {} entries'.format((preds==-1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "SAVE_DIR = 'outputs'\n",
    "data_name = TEST_DIR.split('/')[-1].split('.')[0]\n",
    "dd = os.path.join(SAVE_DIR,data_name+'_FastText')\n",
    "if not os.path.exists(dd):\n",
    "    os.mkdir(dd)\n",
    "dd = os.path.join(dd,'combined_pred_outputs.txt')\n",
    "np.savetxt(dd,preds.astype(int),fmt='%d')\n",
    "print('SAVED TO: {}'.format(dd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## per hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Test Data:\n",
      "\n",
      "TEST_DIR = 'data/FT/sic_hierarchy_c.test.txt'\n",
      "MODEL_DIR = 'FastText/models/190823_195436_sic_hierarchy_c_ova.bin'\n",
      "\n",
      "TEST_DIR = 'data/FT/amazon_hierarchy_2_c.test.txt'\n",
      "MODEL_DIR = 'FastText/models/190825_173230_amazon_hierarchy_2_c_ova.bin'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print existing models\n",
    "print('Existing Test Data:\\n')\n",
    "dd = 'data/FT'\n",
    "md = 'FastText/models'\n",
    "data_dirs = [os.path.join(dd,d) for d in os.listdir(dd) if 'test' in d]\n",
    "model_dirs = [os.path.join(md,d) for d in os.listdir(md) if 'bin' in d]\n",
    "for data_dir in data_dirs:\n",
    "    for model_dir in model_dirs:\n",
    "        if data_dir.split('/')[-1].split('.')[0] in model_dir:\n",
    "            print('TEST_DIR = \\'{}\\''.format(data_dir))\n",
    "            print('MODEL_DIR = \\'{}\\''.format(model_dir))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.97%\r"
     ]
    }
   ],
   "source": [
    "TEST_DIR = 'data/FT/amazon_hierarchy_2_c.test.txt'\n",
    "MODEL_DIR = 'FastText/models/190825_173230_amazon_hierarchy_2_c_ova.bin'\n",
    "true_labels,contents = get_test_data(TEST_DIR)\n",
    "labs = get_all_labels(true_labels)\n",
    "raw_preds,raw_probs = get_prediction(MODEL_DIR,contents,top_k = min(2000,len(labs)),save_preds = False,save_probs= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 286, 2069]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all labels\n",
    "labs = get_all_labels(true_labels)\n",
    "lab_to_ind_dict = get_lab_to_ind_dict(labs,per_hierarchy=True)\n",
    "# get class sizes\n",
    "a, _ =zip(*list(labs))\n",
    "a = np.array(a)\n",
    "cnts = [np.sum(a==i)for i in range(max(a)+1)]\n",
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.97%\r"
     ]
    }
   ],
   "source": [
    "# get topk predictions per H\n",
    "k = 5\n",
    "s = len(raw_preds)\n",
    "preds = [np.ones((s,k))*-1 for _ in cnts]\n",
    "probs = [np.ones((s,k))*-1 for _ in cnts]\n",
    "for i in range(s):\n",
    "    Hs = np.array(list(map(lambda x: lab_to_ind_dict[x][0],raw_preds[i])))\n",
    "    Ps = np.array(list(map(lambda x: lab_to_ind_dict[x][1],raw_preds[i])))\n",
    "    for H in range(len(cnts)):\n",
    "        preds[H][i,:] = Ps[np.argwhere(Hs==H)[:k].flatten()]\n",
    "        probs[H][i,:] = raw_probs[i][np.argwhere(Hs==H)[:k].flatten()]\n",
    "    if i%(s//100)==0:\n",
    "        print('{:.2f}%'.format(i/s*100),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H0 missing 0 entries\n",
      "H1 missing 0 entries\n",
      "H2 missing 0 entries\n"
     ]
    }
   ],
   "source": [
    "for H,pred in enumerate(preds):\n",
    "    print('H{} missing {} entries'.format(H,(pred==-1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save topk predictions per H\n",
    "SAVE_DIR = 'outputs'\n",
    "data_name = TEST_DIR.split('/')[-1].split('.')[0]\n",
    "model_dir = os.path.join(SAVE_DIR,data_name+'_FastText')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "# topk predictions per H\n",
    "for H in range(len(preds)):\n",
    "    np.savetxt(os.path.join(model_dir,'pred_outputs{}.txt'.format(H)),preds[H].astype(int),fmt='%d')\n",
    "    np.savetxt(os.path.join(model_dir,'pred_probs{}.txt'.format(H)),probs[H],fmt='%.4f')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/amazon_hierarchy_2_c_FastText/combined_pred_outputs.txt',\n",
       " 'outputs/amazon_hierarchy_2_c_FastText/pred_outputs0.txt',\n",
       " 'outputs/amazon_hierarchy_2_c_FastText/pred_outputs1.txt',\n",
       " 'outputs/amazon_hierarchy_2_c_FastText/pred_outputs2.txt',\n",
       " 'outputs/amazon_hierarchy_2_c_FastText/pred_probs0.txt',\n",
       " 'outputs/amazon_hierarchy_2_c_FastText/pred_probs1.txt',\n",
       " 'outputs/amazon_hierarchy_2_c_FastText/pred_probs2.txt']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from saved predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_DIR = 'data/FT/sic_hierarchy_c.test.txt'\n",
    "# MODEL_DIR = 'FastText/models/190823_195436_sic_hierarchy_c_ova_pred.pkl'\n",
    "TEST_DIR = 'data/FT/amazon_hierarchy_2_c.test.txt'\n",
    "MODEL_DIR = 'FastText/models/190825_173230_amazon_hierarchy_2_c_ova_pred.pkl'\n",
    "true_labels,contents = get_test_data(TEST_DIR)\n",
    "raw_preds = pickle.load(open(MODEL_DIR,'rb'))\n",
    "preds = get_correct(true_labels,raw_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "print('classification p@k:',end='')\n",
    "print(['{:.2f}'.format(preds[:,:k].any(axis=1).mean()*100) for k in [1,3,4,5]])\n",
    "print('multi-label p@k   :',end='')\n",
    "print(['{:.2f}'.format(preds[:,:k].mean()*100) for k in [1,3,4,5]])\n",
    "print('nDCGAtk           :',end='')\n",
    "dcgs = []\n",
    "dcg= preds/np.log(np.arange(PRED_SIZE)+2)\n",
    "num_labs = len(true_labels[0])\n",
    "for k in [1,3,4,5]:\n",
    "    norm_const = (1/np.log(np.arange(min(k,num_labs))+2)).sum()\n",
    "    dcgs.append(dcg[:,:k].sum(axis=1).mean()/norm_const)\n",
    "print(['{:.2f}'.format(dcg*100) for dcg in dcgs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at stats on each H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at stats on each H\n",
    "for H in range(4):\n",
    "    preds = get_correct(true_labels,raw_preds,'__label__{}_'.format(H))\n",
    "    print('classification p@k:',end='')\n",
    "    print(['{:.2f}'.format(preds[:,:k].any(axis=1).mean()*100) for k in [1,3,4,5]])\n",
    "    dcgs = []\n",
    "    dcg= preds/np.log(np.arange(PRED_SIZE)+2)\n",
    "    num_labs = 1\n",
    "    print('nDCG@k            :',end='')\n",
    "    for k in [1,3,4,5]:\n",
    "        norm_const = (1/np.log(np.arange(min(k,num_labs))+2)).sum()\n",
    "        dcgs.append(dcg[:,:k].sum(axis=1).mean()/norm_const)\n",
    "    print(['{:.2f}'.format(dcg*100) for dcg in dcgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sneaky check of missing preds\n",
    "woop = []\n",
    "for i,raw_pred in enumerate(raw_preds):\n",
    "    woop.append(set([l.split('_')[-2] for l in raw_pred]))\n",
    "nooo = []\n",
    "for i,w in enumerate(woop):\n",
    "    if len(w)<len(true_labels[0]):\n",
    "        nooo.append(i)\n",
    "print(len(nooo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get macro-average p@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total count of each label\n",
    "cnts = defaultdict(int)\n",
    "s = len(true_labels)\n",
    "for i in range(s):\n",
    "    for lab in true_labels[i]:\n",
    "        cnts[lab]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = len(true_labels)\n",
    "for k in [1,4]:\n",
    "    corrAtk = defaultdict(int)\n",
    "    for i in range(s):\n",
    "        for j in range(k):\n",
    "            if raw_preds[i][0] in true_labels[i]:\n",
    "                corrAtk[raw_preds[i][0]] +=1\n",
    "    accAtk = {lab:corrAtk[lab]/cnts[lab] for lab in cnts.keys()}\n",
    "    print((np.array([val for key,val in accAtk.items()])).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get macro-average acc@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total count of each label\n",
    "cnts = defaultdict(int)\n",
    "s = len(true_labels)\n",
    "for i in range(s):\n",
    "    for lab in true_labels[i]:\n",
    "        cnts[lab]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total acc of each label\n",
    "pp = []\n",
    "for H in range(len(true_labels[0])):\n",
    "    pp.append(get_correct(true_labels,raw_preds,'__label__{}_'.format(H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "corrAtk = defaultdict(int)\n",
    "for i in range(s):\n",
    "    for j,lab in enumerate(sorted(true_labels[i])):\n",
    "        corrAtk[lab]+=pp[j][i,:k].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accAtk = {lab:val/cnts[lab] for lab,val in corrAtk.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.array([val for key,val in accAtk.items()])).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for H in range(len(true_labels[0])):\n",
    "    print((np.array([val for key,val in accAtk.items() if '__label__{}_'.format(H) in key])).mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at stats on each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = defaultdict(int)\n",
    "s = len(true_labels)\n",
    "for i in range(s):\n",
    "    for lab in true_labels[i]:\n",
    "        cnts[lab]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = []\n",
    "for H in range(4):\n",
    "    preds = get_correct(true_labels,raw_preds,'__label__{}_'.format(H))\n",
    "    precisions.append([preds[:,:k].any(axis=1).mean()*100 for k in [1,3,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = np.array([sum([1 for l in cnts.keys() if '__label__{}_'.format(H) in l ])/len(cnts) for H in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(precisions)*perc[:,np.newaxis]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p@k per label\n",
    "corr = defaultdict(list)\n",
    "match = None\n",
    "s = len(true_labels)\n",
    "for i in range(s):\n",
    "    k=0\n",
    "    for pred in raw_preds[i]:\n",
    "        if match is not None and match not in pred:\n",
    "            continue\n",
    "        if pred in true_labels[i]:\n",
    "            corr[pred].append(k)\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "pAt = dict()\n",
    "for key,cnt in cnts.items():\n",
    "    if cnt==0:\n",
    "        continue\n",
    "    pAt[key]=(np.array(corr[key])<k).sum()/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{:.2f}'.format(np.array(list(pAt.values())).mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(list(pAt.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labs,_ = get_test_data('data/FT/sic_hierarchy_c.train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnts = defaultdict(int)\n",
    "s = len(train_labs)\n",
    "for i in range(s):\n",
    "    for lab in train_labs[i]:\n",
    "        train_cnts[lab]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labs = sorted(train_cnts.items(), key=lambda kv: kv[1])\n",
    "sl = [l[0] for l in sorted_labs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([pAt[l] for l in sl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
