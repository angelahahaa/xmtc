{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "# import argparse\n",
    "import os,datetime\n",
    "\n",
    "# save things\n",
    "import pandas as pd\n",
    "# from keras.callbacks import CSVLogger\n",
    "\n",
    "# model_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input\n",
    "def get_input(in_dir, mode, sparse = False, get_output = [True]*4):\n",
    "    # get_output: bool of get this var ['x_train','y_trains','x_test','y_tests']\n",
    "    x_train,y_trains,x_test,y_tests = [None]*4\n",
    "    if get_output[0]:\n",
    "        x_train = np.load(os.path.join(in_dir,'x_train.npy'))\n",
    "    if get_output[1]:\n",
    "        dirs = [os.path.join(in_dir,d) for d in sorted(os.listdir(in_dir)) if d.startswith('y_train_{}'.format(mode))]\n",
    "        y_trains = [scipy.sparse.load_npz(d) for d in dirs]\n",
    "        if not sparse:\n",
    "            y_trains = [y.toarray() for y in y_trains]\n",
    "    if get_output[2]:\n",
    "        x_test = np.load(os.path.join(in_dir,'x_test.npy'))\n",
    "    if get_output[3]:\n",
    "        dirs = [os.path.join(in_dir,d) for d in sorted(os.listdir(in_dir)) if d.startswith('y_test_{}'.format(mode))]\n",
    "        y_tests = [scipy.sparse.load_npz(d) for d in dirs]\n",
    "        if not sparse:\n",
    "            y_tests = [y.toarray() for y in y_tests]\n",
    "    return x_train,y_trains,x_test,y_tests\n",
    "\n",
    "def get_bert_input(in_dir,mode):\n",
    "    df = pd.read_pickle(os.path.join(in_dir,'bert_x.pkl'))\n",
    "    train_df = df[df['train/test']=='train']\n",
    "    test_df = df[df['train/test']=='test']\n",
    "    train_sequence = train_df['sequence'].to_list()\n",
    "    train_mask = train_df['mask'].to_list()\n",
    "    train_segment = [[0]*len(train_mask[0]) ]*len(train_mask)\n",
    "    test_sequence = test_df['sequence'].to_list()\n",
    "    test_mask = test_df['mask'].to_list()\n",
    "    test_segment = [[0]*len(test_mask[0]) ]*len(test_mask)\n",
    "    x_trains = [train_sequence,train_mask,train_segment]\n",
    "    x_tests = [test_sequence,test_mask,test_segment]\n",
    "    _,y_trains,_,y_tests = get_input(in_dir, mode, get_output=[0,1,0,1])\n",
    "    return x_trains, y_trains, x_tests, y_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = 'data/sic_hierarchy'\n",
    "x_trains, y_trains, x_tests, y_tests = get_bert_input(IN_DIR,'cat')\n",
    "max_sequence_length = len(x_trains[0][0])\n",
    "labels_dims = [len(y[0]) for y in y_tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build bert layer (original from https://github.com/strongio/keras-bert/blob/master/keras-bert.ipynb)\n",
    "class BertLayer(Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_dim = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "# Build model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_inputs = [\n",
    "    Input(shape=(max_sequence_length,), name=\"input_sequence\"),\n",
    "    Input(shape=(max_sequence_length,), name=\"input_mask\"),\n",
    "    Input(shape=(max_sequence_length,), name=\"input_segment\"),   \n",
    "]\n",
    "bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
    "# dense = Dense(256, activation='relu')(bert_output)\n",
    "# dense = Dropout(0.5)(dense)\n",
    "outs = []\n",
    "for i,labels_dim in enumerate(labels_dims):\n",
    "    outs.append(Dense(labels_dim, activation = None, name = 'H{}'.format(i))(bert_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss(y_true,y_pred):\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true,y_pred,from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    initialize_vars(sess)\n",
    "    model = Model(inputs=bert_inputs, outputs=outs)\n",
    "    model.summary()\n",
    "    model.compile(loss = myloss,\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['acc'])\n",
    "    model.fit(x_trains, y_trains,\n",
    "          batch_size = 256,\n",
    "          epochs = 1,\n",
    "          validation_data = (x_tests, y_tests),\n",
    "#           callbacks = [csv_logger],\n",
    "          shuffle = True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('wop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
