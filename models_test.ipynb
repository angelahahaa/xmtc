{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os,re,datetime\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.metrics import categorical_accuracy, binary_accuracy, top_k_categorical_accuracy\n",
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import scipy.sparse\n",
    "from tools.helper import MetricsAtTopK\n",
    "from tools.MyClock import MyClock\n",
    "from models import get_model\n",
    "clk = MyClock()\n",
    "\n",
    "# argparse\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description = 'run baseline models')\n",
    "parser.add_argument('-i','--input', required = True, type = str, help = 'input directory e.g. ./data/dl_amazon_1/')\n",
    "parser.add_argument('-o','--output', required = True, type = str, help = 'output directory')\n",
    "parser.add_argument('-m','--model', required = True, type = str, help = 'model, one in: xmlcnn, attentionxml, attention,')\n",
    "parser.add_argument('--epoch', default = 5, type = int, help = 'epochs')\n",
    "parser.add_argument('--batch_size', default = 0, type = int, help = 'batch size')\n",
    "parser.add_argument('--early_stopping', default = False, action = 'store_true', help = 'early stopping using validation set (not implemented yet)')\n",
    "parser.add_argument('--save_weights', default = True, action = 'store_true', help = 'save trained model weights')\n",
    "parser.add_argument('--save_prediction', default = 10, type = int, help = 'save top k prediction and corresponding probabilities (not implemented yet)')\n",
    "# args = parser.parse_args('-i data/sic_hiararchy -o woop -m xmlcnn --epoch 5'.split(' '))\n",
    "\n",
    "args = parser.parse_args('-i data/dl_sic -o woop -m xmlcnn --epoch 5'.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READ DATA...\n",
      "Train: 588992, Test: 147247, Labels: [18, 77, 453, 538], Vocab size: 50000, Embedding: 300\n"
     ]
    }
   ],
   "source": [
    "def binary_cross_entropy_with_logits(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_true,y_pred,from_logits=True),axis=-1)\n",
    "def categorical_cross_entropy_with_logits(y_true, y_pred):\n",
    "    return K.mean(K.categorical_crossentropy(y_true,y_pred,from_logits=True),axis=-1)\n",
    "# metrics\n",
    "def binary_accuracy_with_logits(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.tf.cast(K.less(0.0,y_pred), y_true.dtype)))\n",
    "pat1 = MetricsAtTopK(k=1)\n",
    "pat5 = MetricsAtTopK(k=5)\n",
    "def p1(x,y):\n",
    "    return pat1.precision_at_k(x,y)\n",
    "def p5(x,y):\n",
    "    return pat5.precision_at_k(x,y)\n",
    "\n",
    "if not args.batch_size:\n",
    "    if args.model == 'attention':\n",
    "        args.batch_size = 25\n",
    "    elif args.model == 'xmlcnn':\n",
    "        args.batch_size = 128\n",
    "    elif args.model == 'attentionxml':\n",
    "        args.batch_size = 20\n",
    "\n",
    "IN_DIR = args.input\n",
    "OUT_DIR = args.output\n",
    "in_dirs = {\n",
    "    'embedding_matrix':'embedding_matrix.npy',\n",
    "    'x_train':'x_train.npy',\n",
    "    'x_test':'x_test.npy',}\n",
    "for d in os.listdir(IN_DIR):\n",
    "    if d.startswith('y_'):\n",
    "        in_dirs[d.split('.')[0]]=d\n",
    "for key,val in in_dirs.items():\n",
    "    d = os.path.join(IN_DIR,val)\n",
    "    if not os.path.exists(d):\n",
    "        raise Exception('path does not exist: {}'.format(d))\n",
    "    else:\n",
    "        in_dirs[key] = d\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.mkdir(OUT_DIR)\n",
    "out_dir = os.path.join(\n",
    "    args.output,\n",
    "    datetime.datetime.now().strftime('%y%m%d_%H%M%S_{}'.format(args.model)),\n",
    ")\n",
    "\n",
    "# things\n",
    "if not os.path.exists(IN_DIR):\n",
    "    raise Exception('input path does not exist: {}'.format(IN_DIR))\n",
    "print('READ DATA...')\n",
    "embedding_matrix = np.load(in_dirs['embedding_matrix'])\n",
    "x_train = np.load(in_dirs['x_train'])\n",
    "x_test = np.load(in_dirs['x_test'])\n",
    "y_trains = [scipy.sparse.load_npz(d).todense() for key,d in sorted(in_dirs.items()) if key.startswith('y_train')]\n",
    "y_tests = [scipy.sparse.load_npz(d).todense() for key,d in sorted(in_dirs.items()) if key.startswith('y_test')]\n",
    "labels_dims = [y_train.shape[-1] for y_train in y_trains]\n",
    "num_words,embedding_dim = embedding_matrix.shape\n",
    "max_sequence_length = x_train.shape[1]\n",
    "print('Train: {}, Test: {}, Labels: {}, Vocab size: {}, Embedding: {}'.format(\n",
    "    x_train.shape[0],x_test.shape[0],labels_dims,num_words-1,embedding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, LSTM, Dropout\n",
    "from keras.layers import TimeDistributed, Lambda, Softmax, merge\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Input, Embedding\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pAt1(y_true,y_pred):\n",
    "    return categorical_accuracy(y_true, y_pred)\n",
    "def pAt5(y_true,y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 22:43:55.495620 139731949930304 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0722 22:43:55.511729 139731949930304 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0722 22:43:55.800088 139731949930304 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0722 22:43:55.815241 139731949930304 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0722 22:43:55.871634 139731949930304 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0722 22:43:55.877720 139731949930304 deprecation.py:506] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'xmlcnn'\n",
    "bottle_neck = 256\n",
    "num_words,embedding_dim = embedding_matrix.shape\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "if model_name == 'xmlcnn':\n",
    "    filter_sizes = [2,4,8]\n",
    "    pooling_units = 32\n",
    "    convs = []\n",
    "    for fsz in filter_sizes:\n",
    "        l = Conv1D(filters = 128, kernel_size = fsz, strides = 2, activation = 'relu')(embedded_sequences)\n",
    "        s = int(l.shape[-2])\n",
    "        pool_size = s//pooling_units\n",
    "        l = MaxPooling1D(pool_size,padding = 'same')(l)\n",
    "        l = Flatten()(l)\n",
    "        convs.append(l)\n",
    "    x = Concatenate(axis=-1)(convs)\n",
    "    x = Dense(bottle_neck, activation = 'relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outs = []\n",
    "    for i,labels_dim in enumerate(labels_dims):\n",
    "        outs.append(Dense(labels_dim, activation = None, name = 'out{}'.format(i))(x))\n",
    "elif model_name == 'xmlcnn_2':\n",
    "    filter_sizes = [2,4,8]\n",
    "    pooling_units = 32\n",
    "    convs = []\n",
    "    for fsz in filter_sizes:\n",
    "        l = Conv1D(filters = 128, kernel_size = fsz, strides = 2, activation = 'relu')(embedded_sequences)\n",
    "        s = int(l.shape[-2])\n",
    "        pool_size = s//pooling_units\n",
    "        l = MaxPooling1D(pool_size,padding = 'same')(l)\n",
    "        l = Flatten()(l)\n",
    "        convs.append(l)\n",
    "    x = Concatenate(axis=-1)(convs)\n",
    "    outs = []\n",
    "    for i,labels_dim in enumerate(labels_dims):\n",
    "        if labels_dim<bottle_neck:\n",
    "            x2 = Dense(bottle_neck, activation = 'relu')(x)\n",
    "            x2 = Dropout(0.5)(x2)\n",
    "        else:\n",
    "            x2 = x\n",
    "        outs.append(Dense(labels_dim, activation = None, name = 'out{}'.format(i))(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 22:43:55.926267 139731949930304 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0722 22:43:55.942838 139731949930304 deprecation.py:323] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3298: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     15000300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 128)     76928       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 99, 128)      153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 97, 128)      307328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 34, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 33, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 33, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4352)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4224)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4224)         0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12800)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          3277056     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out0 (Dense)                    (None, 18)           4626        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out1 (Dense)                    (None, 77)           19789       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out2 (Dense)                    (None, 453)          116421      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out3 (Dense)                    (None, 538)          138266      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,094,442\n",
      "Trainable params: 4,094,142\n",
      "Non-trainable params: 15,000,300\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(sequence_input, outs)\n",
    "model.compile(loss=categorical_cross_entropy_with_logits,\n",
    "              optimizer='adam',\n",
    "              metrics=[pAt1,pAt5])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 22:43:56.436262 139731949930304 deprecation_wrapper.py:119] From /home/angela/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 588992 samples, validate on 147247 samples\n",
      "Epoch 1/5\n",
      "588992/588992 [==============================] - 90s 153us/step - loss: 9.1625 - out0_loss: 1.3133 - out1_loss: 1.8988 - out2_loss: 2.9116 - out3_loss: 3.0388 - out0_pAt1: 0.6227 - out0_pAt5: 0.8943 - out1_pAt1: 0.5467 - out1_pAt5: 0.7813 - out2_pAt1: 0.4003 - out2_pAt5: 0.6281 - out3_pAt1: 0.3800 - out3_pAt5: 0.6135 - val_loss: 7.5634 - val_out0_loss: 1.1070 - val_out1_loss: 1.5835 - val_out2_loss: 2.3820 - val_out3_loss: 2.4909 - val_out0_pAt1: 0.6782 - val_out0_pAt5: 0.9223 - val_out1_pAt1: 0.6062 - val_out1_pAt5: 0.8374 - val_out2_pAt1: 0.4841 - val_out2_pAt5: 0.7212 - val_out3_pAt1: 0.4633 - val_out3_pAt5: 0.7090\n",
      "Epoch 2/5\n",
      "588992/588992 [==============================] - 87s 147us/step - loss: 7.9716 - out0_loss: 1.1726 - out1_loss: 1.6665 - out2_loss: 2.5102 - out3_loss: 2.6223 - out0_pAt1: 0.6656 - out0_pAt5: 0.9148 - out1_pAt1: 0.5938 - out1_pAt5: 0.8254 - out2_pAt1: 0.4672 - out2_pAt5: 0.7018 - out3_pAt1: 0.4461 - out3_pAt5: 0.6899 - val_loss: 7.3673 - val_out0_loss: 1.0876 - val_out1_loss: 1.5488 - val_out2_loss: 2.3124 - val_out3_loss: 2.4185 - val_out0_pAt1: 0.6832 - val_out0_pAt5: 0.9249 - val_out1_pAt1: 0.6140 - val_out1_pAt5: 0.8435 - val_out2_pAt1: 0.4976 - val_out2_pAt5: 0.7345 - val_out3_pAt1: 0.4756 - val_out3_pAt5: 0.7230\n",
      "Epoch 3/5\n",
      "588992/588992 [==============================] - 87s 148us/step - loss: 7.6610 - out0_loss: 1.1291 - out1_loss: 1.6009 - out2_loss: 2.4118 - out3_loss: 2.5191 - out0_pAt1: 0.6753 - out0_pAt5: 0.9199 - out1_pAt1: 0.6051 - out1_pAt5: 0.8350 - out2_pAt1: 0.4811 - out2_pAt5: 0.7180 - out3_pAt1: 0.4606 - out3_pAt5: 0.7060 - val_loss: 7.3311 - val_out0_loss: 1.0853 - val_out1_loss: 1.5411 - val_out2_loss: 2.2992 - val_out3_loss: 2.4055 - val_out0_pAt1: 0.6878 - val_out0_pAt5: 0.9246 - val_out1_pAt1: 0.6169 - val_out1_pAt5: 0.8460 - val_out2_pAt1: 0.5008 - val_out2_pAt5: 0.7406 - val_out3_pAt1: 0.4801 - val_out3_pAt5: 0.7291\n",
      "Epoch 4/5\n",
      "588992/588992 [==============================] - 87s 147us/step - loss: 7.4535 - out0_loss: 1.0986 - out1_loss: 1.5565 - out2_loss: 2.3459 - out3_loss: 2.4525 - out0_pAt1: 0.6814 - out0_pAt5: 0.9239 - out1_pAt1: 0.6125 - out1_pAt5: 0.8418 - out2_pAt1: 0.4893 - out2_pAt5: 0.7275 - out3_pAt1: 0.4686 - out3_pAt5: 0.7157 - val_loss: 7.3406 - val_out0_loss: 1.0839 - val_out1_loss: 1.5415 - val_out2_loss: 2.3039 - val_out3_loss: 2.4114 - val_out0_pAt1: 0.6884 - val_out0_pAt5: 0.9256 - val_out1_pAt1: 0.6211 - val_out1_pAt5: 0.8474 - val_out2_pAt1: 0.5075 - val_out2_pAt5: 0.7438 - val_out3_pAt1: 0.4866 - val_out3_pAt5: 0.7318\n",
      "Epoch 5/5\n",
      "588992/588992 [==============================] - 87s 147us/step - loss: 7.3135 - out0_loss: 1.0758 - out1_loss: 1.5250 - out2_loss: 2.3032 - out3_loss: 2.4094 - out0_pAt1: 0.6865 - out0_pAt5: 0.9261 - out1_pAt1: 0.6175 - out1_pAt5: 0.8462 - out2_pAt1: 0.4941 - out2_pAt5: 0.7334 - out3_pAt1: 0.4729 - out3_pAt5: 0.7220 - val_loss: 7.3743 - val_out0_loss: 1.0887 - val_out1_loss: 1.5491 - val_out2_loss: 2.3150 - val_out3_loss: 2.4215 - val_out0_pAt1: 0.6896 - val_out0_pAt5: 0.9260 - val_out1_pAt1: 0.6215 - val_out1_pAt5: 0.8481 - val_out2_pAt1: 0.5076 - val_out2_pAt5: 0.7437 - val_out3_pAt1: 0.4876 - val_out3_pAt5: 0.7327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15d4820780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_trains,\n",
    "          batch_size = args.batch_size,\n",
    "          epochs = args.epoch,\n",
    "          validation_data = (x_test, y_tests),\n",
    "#               callbacks = [csv_logger],\n",
    "          shuffle=True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 22:51:14.644289 139731949930304 deprecation.py:323] From /home/angela/env/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model2 = Model(sequence_input, outs)\n",
    "model2.compile(loss=binary_cross_entropy_with_logits,\n",
    "               optimizer='adam',\n",
    "               metrics=[pAt1,pAt5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 588992 samples, validate on 147247 samples\n",
      "Epoch 1/5\n",
      "588992/588992 [==============================] - 94s 160us/step - loss: 0.1374 - out0_loss: 0.0924 - out1_loss: 0.0301 - out2_loss: 0.0079 - out3_loss: 0.0070 - out0_pAt1: 0.6995 - out0_pAt5: 0.9280 - out1_pAt1: 0.6227 - out1_pAt5: 0.8459 - out2_pAt1: 0.4858 - out2_pAt5: 0.7231 - out3_pAt1: 0.4635 - out3_pAt5: 0.7104 - val_loss: 0.1351 - val_out0_loss: 0.0916 - val_out1_loss: 0.0293 - val_out2_loss: 0.0075 - val_out3_loss: 0.0066 - val_out0_pAt1: 0.6978 - val_out0_pAt5: 0.9275 - val_out1_pAt1: 0.6224 - val_out1_pAt5: 0.8454 - val_out2_pAt1: 0.4955 - val_out2_pAt5: 0.7312 - val_out3_pAt1: 0.4737 - val_out3_pAt5: 0.7186\n",
      "Epoch 2/5\n",
      "588992/588992 [==============================] - 93s 158us/step - loss: 0.1293 - out0_loss: 0.0868 - out1_loss: 0.0284 - out2_loss: 0.0075 - out3_loss: 0.0066 - out0_pAt1: 0.7107 - out0_pAt5: 0.9329 - out1_pAt1: 0.6280 - out1_pAt5: 0.8495 - out2_pAt1: 0.4825 - out2_pAt5: 0.7231 - out3_pAt1: 0.4600 - out3_pAt5: 0.7104 - val_loss: 0.1347 - val_out0_loss: 0.0914 - val_out1_loss: 0.0292 - val_out2_loss: 0.0075 - val_out3_loss: 0.0066 - val_out0_pAt1: 0.6996 - val_out0_pAt5: 0.9269 - val_out1_pAt1: 0.6228 - val_out1_pAt5: 0.8444 - val_out2_pAt1: 0.4949 - val_out2_pAt5: 0.7277 - val_out3_pAt1: 0.4736 - val_out3_pAt5: 0.7154\n",
      "Epoch 3/5\n",
      "588992/588992 [==============================] - 94s 159us/step - loss: 0.1260 - out0_loss: 0.0843 - out1_loss: 0.0278 - out2_loss: 0.0074 - out3_loss: 0.0066 - out0_pAt1: 0.7179 - out0_pAt5: 0.9360 - out1_pAt1: 0.6322 - out1_pAt5: 0.8525 - out2_pAt1: 0.4817 - out2_pAt5: 0.7234 - out3_pAt1: 0.4591 - out3_pAt5: 0.7103 - val_loss: 0.1359 - val_out0_loss: 0.0922 - val_out1_loss: 0.0295 - val_out2_loss: 0.0076 - val_out3_loss: 0.0067 - val_out0_pAt1: 0.6983 - val_out0_pAt5: 0.9276 - val_out1_pAt1: 0.6219 - val_out1_pAt5: 0.8443 - val_out2_pAt1: 0.4884 - val_out2_pAt5: 0.7251 - val_out3_pAt1: 0.4669 - val_out3_pAt5: 0.7130\n",
      "Epoch 4/5\n",
      "588992/588992 [==============================] - 94s 159us/step - loss: 0.1233 - out0_loss: 0.0821 - out1_loss: 0.0273 - out2_loss: 0.0073 - out3_loss: 0.0065 - out0_pAt1: 0.7240 - out0_pAt5: 0.9387 - out1_pAt1: 0.6362 - out1_pAt5: 0.8556 - out2_pAt1: 0.4811 - out2_pAt5: 0.7248 - out3_pAt1: 0.4587 - out3_pAt5: 0.7117 - val_loss: 0.1380 - val_out0_loss: 0.0936 - val_out1_loss: 0.0299 - val_out2_loss: 0.0077 - val_out3_loss: 0.0068 - val_out0_pAt1: 0.6976 - val_out0_pAt5: 0.9270 - val_out1_pAt1: 0.6217 - val_out1_pAt5: 0.8438 - val_out2_pAt1: 0.4900 - val_out2_pAt5: 0.7242 - val_out3_pAt1: 0.4689 - val_out3_pAt5: 0.7115\n",
      "Epoch 5/5\n",
      "588992/588992 [==============================] - 93s 158us/step - loss: 0.1212 - out0_loss: 0.0805 - out1_loss: 0.0270 - out2_loss: 0.0073 - out3_loss: 0.0064 - out0_pAt1: 0.7286 - out0_pAt5: 0.9407 - out1_pAt1: 0.6388 - out1_pAt5: 0.8578 - out2_pAt1: 0.4808 - out2_pAt5: 0.7257 - out3_pAt1: 0.4581 - out3_pAt5: 0.7123 - val_loss: 0.1383 - val_out0_loss: 0.0939 - val_out1_loss: 0.0300 - val_out2_loss: 0.0077 - val_out3_loss: 0.0068 - val_out0_pAt1: 0.6973 - val_out0_pAt5: 0.9269 - val_out1_pAt1: 0.6223 - val_out1_pAt5: 0.8431 - val_out2_pAt1: 0.4897 - val_out2_pAt5: 0.7241 - val_out3_pAt1: 0.4683 - val_out3_pAt5: 0.7114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f13a1b43320>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_trains,\n",
    "          batch_size = args.batch_size,\n",
    "          epochs = args.epoch,\n",
    "          validation_data = (x_test, y_tests),\n",
    "#               callbacks = [csv_logger],\n",
    "          shuffle=True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out0_pAt1: 0.4486 - val_out0_pAt5: 0.7201 - val_out1_pAt1: 0.6179 - val_out1_pAt5: 0.8793 - val_out2_pAt1: 0.8841 - val_out2_pAt5: 0.9808\n",
    "            \n",
    "val_out0_pAt1: 0.3923 - val_out0_pAt5: 0.6645 - val_out1_pAt1: 0.6012 - val_out1_pAt5: 0.8614 - val_out2_pAt1: 0.8972 - val_out2_pAt5: 0.9825\n",
    "                        \n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huge increase in the finer layer accuracy, mild decrease in the root accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "csv_logger = CSVLogger(os.path.join(out_dir,'train.log'),append=False)\n",
    "if args.early_stopping:\n",
    "    pass\n",
    "else:\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size = args.batch_size,\n",
    "              epochs = args.epoch,\n",
    "              validation_data = (x_test, y_test),\n",
    "              callbacks = [csv_logger],\n",
    "              shuffle=True,\n",
    "             )\n",
    "if args.save_weights:\n",
    "    model.save_weights(os.path.join(out_dir,'weights.h5'))\n",
    "if args.save_prediction:\n",
    "    print('SAVE PREDICTIONS')\n",
    "    k = args.save_prediction\n",
    "    batch_size = x_test.shape[0]//100\n",
    "    IND_DIR = os.path.join(out_dir,'prediction_{}_ind.txt'.format(k))\n",
    "    LOGITS_DIR = os.path.join(out_dir,'prediction_{}_logits.txt'.format(k))\n",
    "    f_ind = open(IND_DIR,'ab')\n",
    "    f_logits = open(IND_DIR,'ab')\n",
    "    s = x_test.shape[0]\n",
    "    clk.tic()\n",
    "    for i,start in enumerate(range(0,s,batch_size)):\n",
    "        end = min(start+batch_size,s)\n",
    "        x_batch = x_test[start:end,:]\n",
    "        out_probs = model.predict(x_batch)\n",
    "        ind = np.argsort(out_probs,axis=1)[:,-k:]\n",
    "        ind = ind[:,::-1]\n",
    "        logits = np.take_along_axis(out_probs, ind, axis=1)\n",
    "        np.savetxt(f_ind,ind,fmt='%d')\n",
    "        np.savetxt(f_logits,logits,fmt='%1.3f')\n",
    "        print('{:0.0f}% {}'.format(end/s*100,clk.toc(False)),end='\\r')\n",
    "    f_ind.close()\n",
    "    f_logits.close()\n",
    "csv_path = os.path.join(out_dir,'args.csv')\n",
    "pd.DataFrame.from_dict([vars(args)]).to_csv(csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
