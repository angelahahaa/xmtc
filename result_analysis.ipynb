{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.special import softmax\n",
    "from tools.model_func import *\n",
    "import scipy.sparse as sp\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pAtk(y_true,y_pred,k,per_label=False):\n",
    "    y_true_sparse = np.argmax(np.asarray(y_true),axis=1)\n",
    "    Atk = np.any(y_true_sparse[:,np.newaxis]==y_pred[:,:k],axis=1).astype(int)\n",
    "    if per_label:\n",
    "        lab = []\n",
    "        for i in range(y_true.shape[-1]):\n",
    "            lab.append(np.mean(Atk[y_true_sparse==i]))\n",
    "        return np.array(lab)\n",
    "    else:\n",
    "        return np.mean(Atk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(in_dir):\n",
    "    dirs = sorted([os.path.join(in_dir,d) for d in os.listdir(in_dir)])\n",
    "    out_d = defaultdict(list)\n",
    "    for d in dirs:\n",
    "        log_dir = os.path.join(d,'train.log')\n",
    "        args_dir = os.path.join(d,'args.csv')\n",
    "        if not os.path.exists(log_dir) or not os.path.exists(args_dir):\n",
    "            continue\n",
    "        df = pd.read_csv(log_dir)\n",
    "        arg = pd.read_csv(args_dir)\n",
    "        mode = arg.loc[0,'mode']\n",
    "        arg['dir'] = d\n",
    "        df['dir'] = d\n",
    "        out_d[mode].append(df)   \n",
    "        out_d['args'].append(arg)\n",
    "    args = pd.concat(out_d['args'], ignore_index = True, sort = False)\n",
    "    if 'Unnamed: 0' in args.columns:\n",
    "        args.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = 'outputs'\n",
    "args = get_args(IN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manually fix all categorical probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = args\n",
    "df = df[df['mode']=='cat']\n",
    "df = df[df['loss']=='categorical']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sneaky, only use top 10 preds\n",
    "for model_dir in sorted(df.dir.to_list()):\n",
    "    prob_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_probs')])\n",
    "    if prob_dirs:\n",
    "        print('EXISTS: {}'.format(model_dir))\n",
    "        continue\n",
    "    logit_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_logits')])\n",
    "    prob_dirs = [os.path.join(model_dir,'pred_probs{}.txt'.format(i)) for i in range(len(logit_dirs))]\n",
    "    logits = [np.loadtxt(l) for l in logit_dirs]\n",
    "    probs = [softmax(y,axis=1) for y in logits]\n",
    "    for i,prob in enumerate(probs):\n",
    "        np.savetxt(prob_dirs[i],prob,fmt='%1.3f')\n",
    "    print('SAVED: {}'.format(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it properly (requires GPU)\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "for row in df.index:\n",
    "    with tf.Session() as sess:\n",
    "        in_dir, model_dir, model_name, mode = args.loc[row,['input','dir','model','mode']]\n",
    "        if 'attention' in model_name:\n",
    "            continue\n",
    "        print(model_dir)\n",
    "        has_probs = [d for d in os.listdir(model_dir) if d.startswith('pred_probs')]\n",
    "        if has_probs:\n",
    "            print(\"SKIP DIR, prediction exists: {}\".format(' '.join(has_probs)))\n",
    "            continue\n",
    "\n",
    "        # get input\n",
    "        if model_name == 'bert':\n",
    "            _,_,x_tests,y_tests = get_bert_input(in_dir,mode)\n",
    "        else:\n",
    "            _,_,x_tests,y_tests = get_input(in_dir,mode,get_output=[0,0,1,1])\n",
    "        max_sequence_length = len(x_tests[0][0])\n",
    "        labels_dims = [l.shape[-1] for l in y_tests]\n",
    "        if model_name == 'bert':\n",
    "            model = get_bert_model(max_sequence_length, labels_dims,\n",
    "                                bottle_neck = args.bert_bottle_neck,\n",
    "                                trainable_layers = args.bert_trainable_layers,\n",
    "                                sess = sess,\n",
    "                                )\n",
    "        else:\n",
    "            embedding_layer = get_embedding_layer(IN_DIR)\n",
    "            model = get_model(model_name = args.model,\n",
    "                              max_sequence_length = max_sequence_length,\n",
    "                              labels_dims = labels_dims,\n",
    "                              embedding_layer = embedding_layer)\n",
    "\n",
    "        model.load_weights(os.path.join(model_dir,'weights.h5'))\n",
    "        yp = model.predict(x_tests,verbose = 1)\n",
    "        # GET SOFTMAX PER H\n",
    "        y_probs = [softmax(y,axis=1) for y in yp]\n",
    "        ind_dirs = [os.path.join(model_dir,'pred_outputs{}.txt'.format(i)) for i in range(len(y_tests))]\n",
    "        prob_dirs = [os.path.join(model_dir,'pred_probs{}.txt'.format(i)) for i in range(len(y_tests))]\n",
    "        print('SAVE RESULTS')\n",
    "        for i,y_prob in enumerate(y_probs):\n",
    "            ind = np.argsort(y_prob,axis=1)[:,:-11:-1]\n",
    "            prob = np.take_along_axis(y_prob, ind, axis=1)\n",
    "            np.savetxt(ind_dirs[i],ind,fmt='%d')\n",
    "            np.savetxt(prob_dirs[i],prob,fmt='%1.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manually get cascade top 10 predictions and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = args\n",
    "df = df[df['loss']=='masked_categorical']\n",
    "df = df[df['input']=='data/amazon_hierarchy_2']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: try to remove ruplicates and keep largest \n",
    "def get_cascade_sm(y_preds,y_tests,IN_DIR):\n",
    "    parent_to_child = pickle.load(open(os.path.join(IN_DIR,'parent_to_child.pkl'),'rb'))\n",
    "    Hs = len(y_tests)\n",
    "    cascade_sm = [[] for _ in range(Hs)]\n",
    "    cascade_sm[0] = softmax(y_preds[0],axis=1)\n",
    "    for H in range(1,Hs):\n",
    "        print('.',end='')\n",
    "        layer = np.zeros(y_tests[H].shape)\n",
    "        for parent_ind,child_inds in parent_to_child[H-1].items():\n",
    "            child_sm = softmax(y_preds[H][:,child_inds],axis=1)*cascade_sm[H-1][:,[parent_ind]]\n",
    "            layer[:,child_inds]=np.maximum(layer[:,child_inds],child_sm)\n",
    "        cascade_sm[H] = layer\n",
    "    print()\n",
    "    return cascade_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "\n",
    "masked_categorical_inds = df.index\n",
    "for masked_categorical_ind in masked_categorical_inds:\n",
    "    in_dir, model_dir, model_name, mode = args.loc[masked_categorical_ind,['input','dir','model','mode']]\n",
    "    print(model_dir)\n",
    "    has_probs = [d for d in os.listdir(model_dir) if d.startswith('pred_probs')]\n",
    "    if has_probs:\n",
    "        print(\"SKIP DIR, prediction exists: {}\".format(' '.join(has_probs)))\n",
    "        continue\n",
    "    # get input\n",
    "    if model_name == 'bert':\n",
    "        _,_,x_tests,y_tests = get_bert_input(in_dir,mode)\n",
    "    else:\n",
    "        _,_,x_tests,y_tests = get_input(in_dir,mode,get_output=[0,0,1,1])\n",
    "    max_sequence_length = len(x_tests[0][0])\n",
    "    labels_dims = [l.shape[-1] for l in y_tests]\n",
    "    if model_name == 'bert':\n",
    "        model = get_bert_model(max_sequence_length, labels_dims,\n",
    "                            bottle_neck = args.bert_bottle_neck,\n",
    "                            trainable_layers = args.bert_trainable_layers,\n",
    "                            sess = sess,\n",
    "                            )\n",
    "    else:\n",
    "        embedding_layer = get_embedding_layer(in_dir)\n",
    "        model = get_model(model_name = model_name,\n",
    "                          max_sequence_length = max_sequence_length,\n",
    "                          labels_dims = labels_dims,\n",
    "                          embedding_layer = embedding_layer)\n",
    "    model.load_weights(os.path.join(model_dir,'weights.h5'))\n",
    "    yp = model.predict(x_tests,verbose = 1)\n",
    "    print('GET CASCADE SOFTMAX')\n",
    "    y_probs = get_cascade_sm(yp,y_tests,in_dir)\n",
    "    ind_dirs = [os.path.join(model_dir,'pred_outputs{}.txt'.format(i)) for i in range(len(y_tests))]\n",
    "    prob_dirs = [os.path.join(model_dir,'pred_probs{}.txt'.format(i)) for i in range(len(y_tests))]\n",
    "    print('SAVE RESULTS')\n",
    "    for i,y_prob in enumerate(y_probs):\n",
    "        ind = np.argsort(y_prob,axis=1)[:,:-11:-1]\n",
    "        prob = np.take_along_axis(y_prob, ind, axis=1)\n",
    "        np.savetxt(ind_dirs[i],ind,fmt='%d')\n",
    "        np.savetxt(prob_dirs[i],prob,fmt='%1.3f')\n",
    "        p1,p5 = get_pAtk(y_tests[i],ind,1),get_pAtk(y_tests[i],ind,5)\n",
    "        print('H{}, p@1: {:.4f}, p@5: {:.4f}'.format(i,p1,p5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get missing bert predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET MODEL\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Series' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-40a66d663f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                             \u001b[0mbottle_neck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_bottle_neck\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                             \u001b[0mtrainable_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_trainable_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                             )\n\u001b[1;32m     25\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/xmtc/tools/model_func.py\u001b[0m in \u001b[0;36mget_bert_model\u001b[0;34m(max_sequence_length, labels_dims, bottle_neck, trainable_layers, sess)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input_segment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     ]\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_fine_tune_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbottle_neck\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottle_neck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m           \u001b[0;31m# Wrapping `call` function in autograph to allow for dynamic control\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1879\u001b[0m       \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m     \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m     \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/xmtc/tools/model_func.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Select how many layers to fine tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_fine_tune_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mtrainable_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"encoder/layer_{str(11 - i)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Series' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "\n",
    "# in_dir, model_dir, model_name, mode = ['data/amazon_hierarchy_2','outputs/190828_033151_bert','bert','cat']\n",
    "# print(model_dir)\n",
    "# has_probs = [d for d in os.listdir(model_dir) if d.startswith('pred_probs')]\n",
    "# if has_probs:\n",
    "#     print(\"SKIP DIR, prediction exists: {}\".format(' '.join(has_probs)))\n",
    "# # get input\n",
    "# print('GET INPUT')\n",
    "# if model_name == 'bert':\n",
    "#     _,_,x_tests,y_tests = get_bert_input(in_dir,mode)\n",
    "# else:\n",
    "#     _,_,x_tests,y_tests = get_input(in_dir,mode,get_output=[0,0,1,1])\n",
    "# max_sequence_length = len(x_tests[0][0])\n",
    "# labels_dims = [l.shape[-1] for l in y_tests]\n",
    "with tf.Session() as sess:\n",
    "    print('GET MODEL')\n",
    "    if model_name == 'bert':\n",
    "        model = get_bert_model(max_sequence_length, labels_dims,\n",
    "                            bottle_neck = args.bert_bottle_neck,\n",
    "                            trainable_layers = args.bert_trainable_layers,\n",
    "                            sess = sess,\n",
    "                            )\n",
    "    else:\n",
    "        embedding_layer = get_embedding_layer(in_dir)\n",
    "        model = get_model(model_name = model_name,\n",
    "                          max_sequence_length = max_sequence_length,\n",
    "                          labels_dims = labels_dims,\n",
    "                          embedding_layer = embedding_layer)\n",
    "    model.load_weights(os.path.join(model_dir,'weights.h5'))\n",
    "    y_probs = model.predict(x_tests,verbose = 1)\n",
    "    ind_dirs = [os.path.join(model_dir,'pred_outputs{}.txt'.format(i)) for i in range(len(y_tests))]\n",
    "    prob_dirs = [os.path.join(model_dir,'pred_probs{}.txt'.format(i)) for i in range(len(y_tests))]\n",
    "    print('SAVE RESULTS')\n",
    "    for i,y_prob in enumerate(y_probs):\n",
    "        ind = np.argsort(y_prob,axis=1)[:,:-11:-1]\n",
    "        prob = np.take_along_axis(y_prob, ind, axis=1)\n",
    "        np.savetxt(ind_dirs[i],ind,fmt='%d')\n",
    "        np.savetxt(prob_dirs[i],prob,fmt='%1.3f')\n",
    "        p1,p5 = get_pAtk(y_tests[i],ind,1),get_pAtk(y_tests[i],ind,5)\n",
    "        print('H{}, p@1: {:.4f}, p@5: {:.4f}'.format(i,p1,p5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get per label p@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = 'data/sic_hierarchy'\n",
    "_,y_trains,_,y_tests = get_input(mode='cat', in_dir = IN_DIR, get_output= [0,1,0,1])\n",
    "train_samples = [np.sum(y,axis=0) for y in y_trains]\n",
    "freq_orders = [np.argsort(train_sample) for train_sample in train_samples]\n",
    "\n",
    "df_inds = args[args['input']==IN_DIR].index\n",
    "patk = []\n",
    "for df_ind in df_inds:\n",
    "    model_dir, model_name, mode, loss = args.loc[df_ind,['dir','model','mode','loss']]\n",
    "    if mode == 'hierarchy':\n",
    "        continue\n",
    "#     if loss != 'categorical':\n",
    "#         continue\n",
    "    print(model_dir)\n",
    "    pred_dirs = sorted([os.path.join(model_dir,d) for d in os.listdir(model_dir) if d.startswith('pred_outputs')])\n",
    "    p1 = []\n",
    "    p5 = []\n",
    "    for i,pred_dir in enumerate(pred_dirs):\n",
    "        y_true = y_tests[i]\n",
    "        y_pred = np.loadtxt(pred_dirs[i])\n",
    "        p1.append(get_pAtk(y_true,y_pred,1,per_label=True))\n",
    "        p5.append(get_pAtk(y_true,y_pred,5,per_label=True))\n",
    "    patk.append({\n",
    "        'dir':model_dir,\n",
    "        'p@1':p1,\n",
    "        'p@5':p5,\n",
    "        'model':model_name,\n",
    "        'loss':loss\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingaverage(interval, window_size):\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare diff model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (diff plot method to emphisise tail labels)\n",
    "loss = 'categorical'\n",
    "model_inds = [i for i,model in enumerate(patk) if model['loss']==loss]\n",
    "alpha = 0.5\n",
    "for i in range(len(train_samples)):\n",
    "#     if i!=3:\n",
    "#         continue\n",
    "    ind = freq_orders[i]\n",
    "    fig, axes = plt.subplots(len(model_inds)+1,2,figsize=(20,10))\n",
    "    axes[0,0].set_title('H{} p@1, loss: {}'.format(i,loss))\n",
    "    axes[0,1].set_title('H{} p@5, loss: {}'.format(i,loss))\n",
    "    for ax in [axes[-1,0],axes[-1,1]]:\n",
    "        ax.plot(train_samples[i][ind],'r--')\n",
    "        ax.set_ylabel('train counts',color='r')\n",
    "        ax.tick_params(axis='y', labelcolor='r')\n",
    "        ax.set_yscale('log')\n",
    "    for j,model_ind in enumerate(model_inds):\n",
    "        pat1 = movingaverage(patk[model_ind]['p@1'][i][ind],10)\n",
    "        pat5 = movingaverage(patk[model_ind]['p@5'][i][ind],10)\n",
    "        model = patk[model_ind]['model']\n",
    "        axes[j,0].set_ylabel(model)\n",
    "        axes[j,0].plot(pat1,alpha=alpha)\n",
    "        axes[j,1].plot(pat5,alpha=alpha)\n",
    "        axes[j,0].set_ylim(0,1)\n",
    "        axes[j,1].set_ylim(0,1)        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare diff loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compare models with same model diff loss (diff plot method to emphisise tail labels)\n",
    "model_name = 'attentionxml'\n",
    "model_inds = [i for i,model in enumerate(patk) if model['model']==model_name]\n",
    "alpha = 0.5\n",
    "for i in range(len(train_samples)):\n",
    "#     if i!=3:\n",
    "#         continue\n",
    "    ind = freq_orders[i]\n",
    "    fig, axes = plt.subplots(len(model_inds)+1,2,figsize=(20,10))\n",
    "    axes[0,0].set_title('H{} p@1, model: {}'.format(i,model_name))\n",
    "    axes[0,1].set_title('H{} p@5, model: {}'.format(i,model_name))\n",
    "    for ax in [axes[-1,0],axes[-1,1]]:\n",
    "        ax.plot(train_samples[i][ind],'r--')\n",
    "        ax.set_ylabel('train counts',color='r')\n",
    "        ax.tick_params(axis='y', labelcolor='r')\n",
    "        ax.set_yscale('log')\n",
    "    for j,model_ind in enumerate(model_inds):\n",
    "        pat1 = movingaverage(patk[model_ind]['p@1'][i][ind],10)\n",
    "        pat5 = movingaverage(patk[model_ind]['p@5'][i][ind],10)\n",
    "        loss = patk[model_ind]['loss']\n",
    "        axes[j,0].set_ylabel(loss)\n",
    "        axes[j,0].plot(pat1,alpha=alpha)\n",
    "        axes[j,1].plot(pat5,alpha=alpha)\n",
    "        axes[j,0].set_ylim(0,1)\n",
    "        axes[j,1].set_ylim(0,1)        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot on one graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare models with same loss diff model\n",
    "loss = 'categorical'\n",
    "model_inds = [i for i,model in enumerate(patk) if model['loss']==loss]\n",
    "alpha = 0.5\n",
    "for i in range(len(train_samples)):\n",
    "    ind = freq_orders[i]\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(20,10))\n",
    "    ax1.set_title('H{} p@1, loss: {}'.format(i,loss))\n",
    "    ax2.set_title('H{} p@5, loss: {}'.format(i,loss))\n",
    "    for ax in [ax1.twinx(),ax2.twinx()]:\n",
    "        ax.plot(train_samples[i][ind],'r--')\n",
    "        ax.set_ylabel('train counts',color='r')\n",
    "        ax.tick_params(axis='y', labelcolor='r')\n",
    "        ax.set_yscale('log')\n",
    "    for model_ind in model_inds:\n",
    "        pat1 = patk[model_ind]['p@1'][i]\n",
    "        pat5 = patk[model_ind]['p@5'][i]\n",
    "        model = patk[model_ind]['model']\n",
    "        ax1.plot(pat1[ind],alpha=alpha,label=model)\n",
    "        ax2.plot(pat5[ind],alpha=alpha,label=model)\n",
    "    for ax in [ax1,ax2]:\n",
    "        ax.set_ylim(0,1)\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compare models with same model diff loss\n",
    "model_name = 'attentionxml'\n",
    "model_inds = [i for i,model in enumerate(patk) if model['model']==model_name]\n",
    "alpha = 0.5\n",
    "for i in range(len(train_samples)):\n",
    "    ind = freq_orders[i]\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(20,10))\n",
    "    ax1.set_title('H{} p@1, model: {}'.format(i,model_name))\n",
    "    ax2.set_title('H{} p@5, model: {}'.format(i,model_name))\n",
    "    for ax in [ax1.twinx(),ax2.twinx()]:\n",
    "        ax.plot(train_samples[i][ind],'r--')\n",
    "        ax.set_ylabel('train counts',color='r')\n",
    "        ax.tick_params(axis='y', labelcolor='r')\n",
    "        ax.set_yscale('log')\n",
    "    for model_ind in model_inds:\n",
    "        pat1 = patk[model_ind]['p@1'][i]\n",
    "        pat5 = patk[model_ind]['p@5'][i]\n",
    "        loss = patk[model_ind]['loss']\n",
    "        ax1.plot(pat1[ind],alpha=alpha,label=loss)\n",
    "        ax2.plot(pat5[ind],alpha=alpha,label=loss)\n",
    "    for ax in [ax1,ax2]:\n",
    "        ax.set_ylim(0,1)\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in patk:\n",
    "    print(model['model'],model['loss'])\n",
    "#     for i in range(len(train_samples)):\n",
    "#         p1 = np.mean(model['p@1'][i])\n",
    "#         p5 = np.mean(model['p@5'][i])\n",
    "#         print('p@1/l:{:.6f} p@5/l: {:.6f}'.format(p1,p5))\n",
    "    p1 = np.mean(np.concatenate(model['p@1']))\n",
    "    p5 = np.mean(np.concatenate(model['p@5']))\n",
    "    print('p@1/l:{:.6f} p@5/l: {:.6f}'.format(p1,p5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
